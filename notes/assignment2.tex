\documentclass[10pt,a4paper]{article}

\usepackage{kerkis}
\usepackage[T1]{fontenc}
\usepackage{alphalph}
\usepackage[utf8x]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18, width=10cm}

\usepackage{ucs}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{url}
\usepackage{amssymb}
\usepackage{accents}
\usepackage{xfrac}
\geometry{margin=2cm}
\usepackage{multirow}
\usepackage{caption}
\usepackage{hyperref}
\usepgfplotslibrary{fillbetween}
\captionsetup[table]{position=bottom} 
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue}


\pagestyle{fancy}
\title{2nd Homework Assignment \\ \huge{Project on Support Vector Machines}}
\author{Vasileios Papageorgiou}
\date{\today}
\fancyhead[L]{Optimization Techniques}
\fancyhead[R]{MSc Program in Data Science (PT)}

\newcounter{para}
\newcommand\mypara{\par\refstepcounter{para}\textbf{\thepara.}\space}
\setlength\parindent{0pt}

\renewcommand{\thesubsection}{(\alphalph{\value{subsection}})}

\begin{document}
	\maketitle
	\thispagestyle{fancy}
	
	\lstset{
		language=Python,
		basicstyle=\ttfamily,
		keywordstyle=\color{blue},
		commentstyle=\color{green},
		stringstyle=\color{red},
%%		numbers=left,
%%		# numberstyle=\tiny\color{gray},
		stepnumber=1,
		numbersep=5pt,
		showspaces=false,
		showstringspaces=false,
		frame=single,
		breaklines=true,
		tabsize=4
	}




\section{Introduction}
\label{sec:intro}
The purpose of this project is to implement John Plattâ€™s Sequential Minimal Optimization Algorithm (SMO) to train a Support Vector Machine (SVM) for binary classification. Support Vector Machines are supervised learning models used for classification and regression. Their basic principle involves finding the hyperplane that best separates data points of different classes by maximizing the margin between them. This is achieved by solving an optimization problem to identify the support vectors, which are the data points closest to the hyperplane.

The fundamental principle of SMO is that it tries to solve the dual rather than the primal optimization problem. In this project, we will implement a basic version of the SMO algorithm to train an SVM binary classifier using the methodology from Platt's original paper \cite{platt1998sequential}. We will explain each step of the process, providing the analytical background along with code snippets.

\section{Problem Formulation}
\label{sec:problem}

Given a dataset with \( m \) training examples and \( n \) features, where the \( i \)-th example is represented as \( (x^{(i)}, y^{(i)}) \) with \( y^{(i)} \in \{-1, +1\} \) as the label, we aim to address the linear separation problem, potentially involving outliers. The primal problem involves finding a vector \( \mathbf{w} = (w_1, w_2, \ldots, w_n) \) and a scalar \( b \) under the following constraints:

\[
\begin{aligned}
	& \min \quad \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^{m} s_i \\
	& \text{s.t.} \quad y^{(i)} (\mathbf{w}^T x^{(i)} + b) \geq 1 - s_i, \quad i = 1, \ldots, m \\
	& \quad \quad \quad s_i \geq 0, \quad i = 1, \ldots, m
\end{aligned}
\]

Here, \( C \) is the regularization parameter that penalizes the outliers. We allow training samples to have a margin less than 1 and we pay the cost of \( C s_i\). The parameter controls the trade-off between maximizing the margin and minimizing the margin violations, balancing a wider margin with a smaller number of margin failures.

Using Lagrange duality we can derive the dual problem, corresponding to the primal one, which is formulated as follows:

\[
\begin{aligned}
	& \max_{\alpha} \quad W(\alpha) = \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle \\
	& \text{s.t.} \quad \sum_{i=1}^{m} \alpha_i y^{(i)} = 0 \\
	& \quad \quad \quad 0 \leq \alpha_i \leq C, \quad i = 1, \ldots, m
\end{aligned}
\]

We note that the slack variables \(s_i\) do not appear in the dual formulation.
The optimization problem is now converted into a dual quadratic problem, where the objective function is solely dependent on a set of Lagrange multipliers \(\alpha_i\), under the box constraint \(0 \leq \alpha_i \leq C\) and one linear equality constraint \(\sum_{i=1}^{m} \alpha_i y^{(i)} = 0\). 


The Karush-Kuhn-Tucker (KKT) conditions for the dual problem are for all \( i \):
\[ 
\alpha_i = 0 \iff y_i u_i \geq 1,
\]
\[ 
0 < \alpha_i < C \iff y_i u_i = 1,
\]
\[ 
\alpha_i = C \iff y_i u_i \leq 1.
\]
where \( u_i \) is the output of the SVM for the \( i \)-th training example.

The KKT conditions reveal crucial insights for SVM training:
Support vectors are points where \( 0 < \alpha_i < C \), situated on the margin's edge. These vectors play a critical role in defining the decision boundary.
When \( \alpha_i = 0 \), the point lies outside the margin and has no impact on the prediction, serving as non-support.
Points with \( \alpha_i = C \) are inside the margin and may or may not be classified correctly, influencing the SVM's decision boundary.

We anticipate that only a few points will be support vectors (non-bound points), significantly reducing the amount of calculations required.

There is a one-to-one relationship between each Lagrange multiplier \( \alpha_i \) and each training example. Once the Lagrange multipliers are determined, the normal vector \( \mathbf{w} \) can be derived from them:
\[ 
\mathbf{w} = \sum_{i=1}^{N} y_i \alpha_i \mathbf{x}_i 
\].

\section{SMO Algorithm}
\label{sec:smo}

The Sequential Minimal Optimization (SMO) algorithm tackles the SVM training process by focusing on solving the smallest possible optimization problems step by step. Because of the linear constraint, in each step we must always select a pair of Lagrangian multipliers \(\alpha_i\) and jointly optimize them. What makes SMO efficient is its ability to solve these pairs analytically, avoiding complex numerical optimizations. 
The algorithm is built on two main parts: a method for solving these pairs analytically and a heuristic rule for deciding which pairs to optimize next. 

In our implementation we have followed the exact steps from Platt's paper. We choose to follow an object oriented approach, creating a class named SVM\_classifier that includes the methods \textit{take\_step}, or The pair optimization step, \textit{examine\_example}, that uses heuristics to identify an eligible pair, and \textit{fit} method that corresponds to the main training loop. The theoretical background and the implementation of those methods will be explained in detail below.

\subsection{SMO implementation Details}

\subsubsection{Initial setup}
Firstly, we initialize the classifier class setting the default parameters, required by the algorithm.

\begin{lstlisting}
class SVM_classifier:
	def __init__(self, X, y, kernel:str ='linear', 
				 C:float =1, epsilon:float =1e-8,
				 tol:float = 0.001, max_iter:int= 500):
		self.X = X
		self.y = y
		self.kernel = kernel
		self.kernel_func = self.select_kernel(self.kernel)
		self.C = C
		self.epsilon = epsilon # error margin 
		self.tol = tol # tolerance for KKT
		self.max_iter = max_iter
		self.m, self.n = np.shape(self.X) 
		self.alphas = np.zeros(self.m)
		self.Error_cache = np.zeros(self.m)

		self.w = np.zeros(self.n)
		self.b = 0
\end{lstlisting}

X and y denote the training samples and the corresponding labels, and C is the regularization parameter. We also set a small error variable $\epsilon$, as well as a tolerance within the KKT conditions should be satisfied. 
In the `alphas` array we store the Lagrange multipliers. Here are initialized with zeros. The output of the training process will be the vector w and the threshold b.

A kernel function that measures the similarity or distance between the input and the training vectors is also used. The default is \textit{linear}, which suffices for this project, but we could use also others. Apart from the linear we have also implement an RBF Gaussian kernel:
\newpage
\begin{lstlisting}
def linear_kernel(self, x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
		return x1 @ x2.T

def rbf_kernel(self, x1, x2):

	gamma = 1
	if np.ndim(x1) == 1: x1 = x1[np.newaxis, :]
	if np.ndim(x2) == 1: x2 = x2[np.newaxis, :]

	dist_squared = np.linalg.norm(x1[:, :, np.newaxis] - \
				   x2.T[np.newaxis, :, :], axis=1) ** 2
	dist_squared = np.squeeze(dist_squared)

	return np.exp(-gamma * dist_squared)
\end{lstlisting}

\subsubsection{\textit{take\_step} method}

The method takes as input two points $i_1, i_2$ and takes an optimization step. If that is successful the corresponding parameter arrays are updated else nothing happens and we try another pair.

In order to solve for the two Lagrange multipliers, SMO first computes the constraints on these multipliers and then solves for the constrained minimum.  The inequality constraints cause the Lagrange multipliers to lie within a \([0,C]\) box, while the linear equality constraint causes the
Lagrange multipliers to lie on a diagonal line.

The latter constraint explains why two is the minimum number of Lagrange multipliers that can be optimized together; if SMO optimized only one multiplier, it could not fulfill the linear equality constraint at every step.
The algorithm first computes the second Lagrange multiplier \(\alpha_2\) and computes the ends of the diagonal line segment in terms of it. The ends of the diagonal line segment can be expressed quite simply, as shown below:
\[
L = \left\{
\begin{array}{ll}
	\max(0, \alpha_2 - \alpha_1), & \text{if } y_1 \neq y_2 \\
	\max(0, \alpha_2 + \alpha_1 - C), & \text{if } y_1 = y_2
\end{array}
\right.
\quad
H = \left\{
\begin{array}{ll}
	\min(C, C + \alpha_2 - \alpha_1), & \text{if } y_1 \neq y_2 \\
	\min(C, \alpha_2 + \alpha_1), & \text{if } y_1 = y_2
\end{array}
\right.
\]

Those serve as bounds for \(\alpha_2\). Within the \textit{take\_step} method this is implemented as follows:

\begin{lstlisting}
if y1!=y2:
	L = max(0,a2-a1)
	H = min(self.C,self.C+a2-a1)
else:
	L = max(0,a2+a1-self.C)
	H = min(self.C,a2+a1)
	
if L==H:
	return 0
\end{lstlisting}
	
If both ends are the same, we exit the method and continue with a different pair.

The second derivative of the objective function along the diagonal line is given by:
\[
\eta = K(x_1, x_1) + K(x_2, x_2) - 2 K(x_1, x_2).
\]
Under normal circumstances, where the objective function is \textbf{positive definite}, there exists a minimum along the direction of the linear equality constraint, and \( \eta > 0 \). In such cases, SMO computes the minimum along this constraint direction as:
\[
\alpha_{2,\text{new}} = \alpha_2 + \frac{y_2 (E_1 - E_2)}{\eta},
\]
where \( E_i = u_i - y_i \) represents the error on the \( i \)-th training example and we save it in an \textit{Error\_cache} array.
The constrained minimum is then determined by clipping the unconstrained minimum to the boundaries of the line segment:
\[
\alpha_{2,\text{new,clipped}} =
\begin{cases}
	H, & \text{if } \alpha_{2,\text{new}} \geq H \\
	\alpha_{2,\text{new}}, & \text{if } L < \alpha_{2,\text{new}} < H \\
	L, & \text{if } \alpha_{2,\text{new}} \leq L.
\end{cases}
\]


The corresponding code snippet is as follows:

\begin{lstlisting}
k11 = self.kernel_func(x1,x1)
k22 = self.kernel_func(x2,x2)
k12 = self.kernel_func(x1,x2)
	
# Compute the second derivative along diagonal
eta = k11 + k22 - 2.0*k12
	
if eta > 0:

	a2_new = a2 + y2*(E1-E2)/eta	
	if a2_new>=H:
		a2_new = H
	if a2_new<=L:
		a2_new = L
\end{lstlisting}

Under unusual circumstances, \( \eta \) will not be positive. In such cases, the objective function \( \Psi \) should be evaluated at each end of the line segment by the following equations:
\[
\begin{aligned}
	f_1 &= y_1 (E_1 + b) - \alpha_1 K(x_1, x_1) - s \alpha_2 K(x_1, x_2), \\
	f_2 &= y_2 (E_2 + b) - s \alpha_1 K(x_1, x_2) - \alpha_2 K(x_2, x_2), \\
	L_1 &= \alpha_1 + s(\alpha_2 - L), \\
	H_1 &= \alpha_1 + s(\alpha_2 - H),\\
	\Psi_L &= L1 f_1 + L f_2 + \frac{1}{2} L^2 K(x_1, x_1) + \frac{1}{2} L^2 K(x_2, x_2) + s L L_1 K(x_1, x_2), \\
	\Psi_H &= H1 f_1 + H f_2 + \frac{1}{2} H^2 K(x_1, x_1) + \frac{1}{2} H^2 K(x_2, x_2) + s H H_1 K(x_1, x_2).
\end{aligned}
\]
SMO will move the Lagrange multipliers to the endpoint that yields the lowest value of the objective function \( \Psi \). If the objective function is equal at both ends (within a small \( \epsilon \) for round-off error) the joint minimization cannot make further progress, and the method is exited. Below is the relevant code:

\begin{lstlisting}
f1 = y1*(E1 + self.b) - a1*k11 - s*a2*k12
f2 = y2*(E2 + self.b) - s*a1*k12 - a2*k22
L1 = a1 + s*(a2 - L)
H1 = a1 + s*(a2 - H)
psi_L = L1*f1 + L*f2 + 0.5*L1*L1*k11 + 0.5*L*L*k22 + s*L*L1*k12
psi_H = H1*f1 + H*f2 + 0.5*H1*H1*k11 + 0.5*H*H*k22 + s*H*H1*k12
	
if psi_L < (psi_H - self.epsilon):
	a2_new = L
elif psi_L > (psi_H + self.epsilon):
	a2_new = H
else:
	a2_new = a2

if np.abs(a2_new - a2) < (self.epsilon * (a2_new + a2 + self.epsilon)):
	return 0
\end{lstlisting}

Now we can calculate the The value of \( \alpha_1 \) based on the new clipped \( \alpha_2 \) using the following equation (let \( s = y_1 y_2 \)):

\[
\alpha_{1,\text{new}} = \alpha_1 + s (\alpha_2 - \alpha_{2,\text{new,clipped}}).
\]

The threshold \( b \) is recalculated after each optimization step to satisfy the KKT conditions for both optimized examples. \( b1 \) ensures the SVM outputs \( y1 \) when the input is \( x1 \):
\[
b_1 = E_1 + y1 (\alpha_{1,\text{new}} - \alpha_1) K(x_1, x_1) + y2 (\alpha_{2,\text{new,clipped}} - \alpha_2) K(x_1, x_2) + b.
\]
Similarly, \( b_2 \) ensures the SVM outputs \( y_2 \) when the input is \( x_2 \):
\[
b_2 = E_2 + y_1 (\alpha_{1,\text{new}} - \alpha_1) K(x_1, x_2) + y_2 (\alpha_{2,\text{new,clipped}} - \alpha_2) K(x_2, x_2) + b.
\]
When both \( b_1 \) and \( b_2 \) are valid and equal, or when both new Lagrange multipliers are at bounds (and \( L \neq H \)), any threshold between \( b_1 \) and \( b_2 \) satisfies the KKT conditions. SMO selects the average between \( b_1 \) and \( b_2 \).

For linear kernel, we can easily update and store the the weight vector \( w \) using the following:
\[
w_{\text{new}} = w + y_1 (\alpha_{1,\text{new}} - \alpha_1) x_1 + y_2 (\alpha_{2,\text{new,clipped}} - \alpha_2) x_2.
\]

For all non boundary elements we can also update efficiently the error cache with the help of some algebra and it yields:

\[
E^{new}_i = E^{old}_i + y_1 (\alpha^{new}_1 -
\alpha^{old}_1) K_{1i} + y_2 (\alpha^{new}_2 - \alpha^{old}_2) K_{2i} +
(b_{new} - b)\]

The method exits successful and all the relevant arrays are updated with their new values:

\begin{lstlisting}
# Update threshold b
b1 = self.b + E1 + y1*(a1_new - a1)*k11 + y2*(a2_new - a2)*k12
b2 = self.b + E2 + y1*(a1_new - a1)*k12 + y2*(a2_new - a2)*k22
	
if 0 < a1_new < self.C:
	b_new = b1
elif 0 < a2_new < self.C:
	b_new = b2
else:
	b_new = 0.5*(b1 + b2)
	
# Update weight's vector if Linear kernel
if self.kernel == 'linear':
	self.w = self.w + y1*(a1_new - a1)*x1 + y2*(a2_new - a2)*x2
	
# Update Error_cache using alphas (see reference)
	
# if a1 & a2 are not at bounds, the error will be 0
self.Error_cache[i1] = 0
self.Error_cache[i2] = 0
	
# Update error for non boundary elements
inner_indices = [idx for idx, a in enumerate(self.alphas) if 0 < a < self.C]
for i in inner_indices:
	self.Error_cache[i] += \
	y1*(a1_new - a1)*self.kernel_func(x1, self.X[i,:]) \
	+ y2*(a2_new - a2)*self.kernel_func(x2, self.X[i,:]) \
	+ (self.b - b_new)
	
# Update alphas
	self.alphas[i1] = a1_new
	self.alphas[i2] = a2_new
	
# Update b
	self.b = b_new
	
return 1
	
\end{lstlisting}

\subsubsection{\textit{examine\_example} method}

The SMO algorithm uses two heuristics for choosing Lagrange multipliers. The outer loop first iterates over the entire training set to find examples violating the KKT conditions. Violating examples are then optimized. 

After a full pass, the outer loop focuses on non-bound examples (multipliers neither 0 nor \(C\)), checking and optimizing those that violate the KKT conditions. This process alternates between full passes and non-bound passes until all examples meet the KKT conditions within a tolerance \(\epsilon\), terminating the algorithm.

The first heuristic prioritizes non-bound examples, which are more likely to violate the KKT conditions. The SMO algorithm iterates over the non-bound subset until it is consistent, then scans the entire data set to check bound examples.

For the second multiplier, SMO maximizes the step size by approximating \(| E1 - E2 |\). It chooses errors to maximize the step size, using a cached error value \(E\) for each non-bound example. If no progress is made, SMO uses a hierarchy of second-choice heuristics, iterating through non-bound examples or the entire set to find a pair of multipliers that allow a positive step size. If still unsuccessful, SMO skips the first example and continues with another.

This methodology with the hierarchical heuristics is implemented as follows:

\begin{lstlisting}
def examine_example(self, i2:int=None):

	y2 = self.y[i2]
	a2 = self.alphas[i2]
	E2 = self.get_error(i2)
	r2 = E2 * y2

	# Check if KKT satisfation error is within tolerance
	if (r2 < -self.tol and a2 < self.C) or (r2 > self.tol and a2 > 0):

	# Check if multiple non-bound elements and use 2nd heuristic.
		if np.count_nonzero((0 < self.alphas) & (self.alphas < self.C)) > 1:
	
			if E2 > 0:
				i1 = np.argmin(self.Error_cache)
			else:
				i1 = np.argmax(self.Error_cache)
	
			if self.take_step(i1, i2):
				return 1 # succesfull pass
	
	# Loop over all non-boundary elements, starting at a random point
	# Get indices where 0 < alpha < self.C
	i1_array = np.where((0 < self.alphas) & (self.alphas < self.C))[0]
	
	# Iterate through all starting at random
	random_shift = np.random.choice(np.arange(self.m))
	i1_list = np.roll(i1_array, random_shift)
	
	# Loop over all non-boundary elements
	for i1 in i1_list:
		if self.take_step(i1, i2):
			return 1
	
	# Loop over ALL elements, starting at a random point
	i1_list = np.roll(np.arange(self.m), np.random.choice(np.arange(self.m)))
	for i1 in i1_list:
		if self.take_step(i1, i2):
			return 1
	
	return 0
	
\end{lstlisting}
	
\subsubsection{\textit{fit} method}
This is the main training routine of the program.
The \textit{fit} method by initializing \textit{iteration\_number} to count iterations, \textit{numbers\_changed} to track changes in Lagrange multipliers at every pass, and \textit{examine\_all} to indicate whether all training examples should be examined or not. The main loop continues as long as there were still changes in the Lagrange multipliers or if all examples should be examined (e.g. in the first iteration. If the iteration count exceeds the maximum allowed, the algorithm terminates to avoid infinite loops. 

The algorithm focuses on examining only the subset of non-boundary examples if possible, because they are less in number and that can speed the computation time. If no changes occur in the \(\alpha\) values of those, the algorithm switches to examining all. This alternation between examining all examples and the subset continues until convergence criteria are met (KKT conditions met for all samples within a tolerance), ensuring efficient optimization of the SVM. The implementations is as follows:\newpage

\begin{lstlisting}
def fit(self) -> None:

	iteration_number = 0
	numbers_changed = 0
	examine_all = True
	
	while numbers_changed > 0 or examine_all:

		if iteration_number >= self.max_iter:
		break
	
		numbers_changed = 0
		if examine_all:	
			for i in range(self.m):
				numbers_changed += self.examine_example(i)
	
		else: 
			i_array = np.where((0 < self.alphas) & (self.alphas < self.C))[0]
			for i in i_array:
				numbers_changed += self.examine_example(i)
	
	if examine_all:
		examine_all = False
	if numbers_changed == 0:
		examine_all = True
	
	iteration_number += 1
	
\end{lstlisting}



\section{Experiments on gisette dataset}

In this section, we will use a well known dataset for training and evaluating the SMO implementation. The dataset of choice is \textbf{gisette}.


\subsection{The Gisette Dataset}

The Gisette dataset is a well-known benchmark in machine learning, specifically used for feature selection and binary classification tasks. Originally part of the NIPS 2003 feature selection challenge, this dataset consists of handwritten digit images where the objective is to distinguish between the digits '4' and '9'.

\subsubsection{Dataset Characteristics}

\begin{itemize}
	\item \textbf{Features:} 5000 features, many of which are redundant or irrelevant (noise), making it an excellent test for feature selection algorithms.
	\item \textbf{Instances:} 7000 instances in the training set and 1000 instances in the test set.
	\item \textbf{Classes:} Binary labels representing the digits '4' and '9', which appear very similar when handwritten and pose a difficulty for machine learning algorithms to distinguish them.
\end{itemize}

The Gisette dataset is frequently utilized to evaluate the performance of various machine learning algorithms, especially those designed for high-dimensional data. Due to its high dimensionality and the presence of irrelevant features, it provides a challenging testbed for these algorithms.

Each line of the dataset is formatted as follows:
\begin{verbatim}
	-1 1:-1 2:-1 3:0.913914 4:-1 5:-1 6:0.4530 ...
\end{verbatim}
\begin{itemize}
	\item The first number is the label \( y \) which is either 1 or -1.
	\item It is followed by 5000 pairs of the form \texttt{integer\_index:float\_value} where the floats represent the feature values \( x \).
\end{itemize}

All features are scaled, so no further pre-processing is required. The dataset is loaded using the custom-made function \textit{read\_gisette\_data} from the utils.py module.


\subsubsection{Optimization and Fine-Tuning}

Even though the \texttt{SVM\_classifier} class has the ability to use different types of kernels, for the gisette dataset a simple linear kernel is enough, as the dataset exhibits strong linear separability. However, we could fine-tune the hyper-parameter \( C \). We use a simple k-fold validation procedure on the training dataset choosing five folds and \( C \) range in \([0.1, 1, 10, 100]\). The scoring criterion can be one of accuracy, precision, F1 score, or recall, selected as needed. This function is implemented by the \texttt{tune\_hyperparameter\_C} function from the \texttt{utils.py} module.

After running the algorithm, using accuracy as the scoring metric, we conclude that \( C=0.1 \) is the best for this dataset with a linear kernel. We then train the model using the whole training dataset and evaluate it using the test dataset. We obtain the following results for the metrics:

\begin{itemize}
	\item \textbf{Accuracy of SVM on test set:} 0.9670
	\item \textbf{Recall of SVM on test set:} 0.9640
	\item \textbf{Precision of SVM on test set:} 0.9698
	\item \textbf{F1 of SVM on test set:} 0.9669
\end{itemize}

The custom-made SMO algorithm achieves results similar to those of optimized Python libraries, albeit with slightly slower performance.

\section{Documentation for running the code}

This section gives instructions on how to run the code accompanying this assignment. Similar instructions can be found at the README.md file.

\subsection{Prerequisites}
Before running the code, ensure Python is installed on your system. It's recommended to use a virtual environment for managing dependencies.

\subsection{Installation}
Install required Python packages using pip and the provided \texttt{requirements.txt} file:

\begin{verbatim}
	pip install -r requirements.txt
\end{verbatim}

\subsection{Dataset}
The Gisette dataset files (\texttt{gisette\_scale.bz2} and \texttt{gisette\_scale.t.bz2}) are located in the \texttt{data} directory within the project folder. These files contain pre-scaled and preprocessed data ready for training and testing.

\subsection{Run the code}
Execute the main script to train the SVM classifier:

\begin{verbatim}
	python main.py
\end{verbatim}

\subsubsection{Hyperparameter Tuning (Optional)}
To perform hyperparameter tuning for the SVM classifier:

Follow the prompts to choose whether to perform hyperparameter tuning. If selected, the script will execute and output the best hyperparameters (C) found during tuning.

\subsubsection{Training the SVM Classifier}
Once you have the best hyperparameters (or if you choose not to tune), proceed to train the SVM classifier:

Follow the prompts to specify the kernel type (linear or rbf) and the value of C.

\subsubsection{Viewing Results}
After training, the script will evaluate the classifier on the test set and print the following performance metrics:

\begin{itemize}
	\item Accuracy
	\item Precision
	\item Recall
	\item F1 Score
\end{itemize}

 
\begin{thebibliography}{1}
	
	\bibitem{platt1998sequential}
	John Platt.
	\newblock Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.
	\newblock Technical Report MSR-TR-98-14, Microsoft, April 1998.
	\newblock \href{https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/}{link}
	
	\bibitem{mak2000smo}
	Ginny Mak.
	\newblock The Implementation of Support Vector Machines Using the Sequential Minimal Optimization Algorithm.
	\newblock Master's thesis, McGill University, School of Computer Science, Montreal, Canada, April 2000.
	
\end{thebibliography}



\end{document}