{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-21T13:53:28.384139Z",
     "start_time": "2024-06-21T13:53:28.288294Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#import bz2file as bz2\n",
    "import os\n",
    "from typing import Tuple, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to the Gisette Dataset\n",
    "\n",
    "The Gisette dataset is a well-known benchmark dataset in the field of machine learning, particularly used for feature selection and binary classification tasks. It was originally part of the NIPS 2003 feature selection challenge. The dataset consists of handwritten digit images, where the task is to distinguish between the digits '4' and '9'.\n",
    "\n",
    "## Dataset Characteristics\n",
    "\n",
    "- **Features:** The dataset contains 5000 features, many of which are redundant or irrelevant, making it a good test for feature selection algorithms.\n",
    "- **Instances:** There are 7000 instances in the training set and 1000 instances in the test set.\n",
    "- **Classes:** The labels are binary, with two classes representing the digits '4' and '9'.\n",
    "\n",
    "## Usage\n",
    "\n",
    "The Gisette dataset is often used to evaluate the performance of various machine learning algorithms, especially those designed for high-dimensional data. It provides a challenging testbed for algorithms due to its high dimensionality and the presence of irrelevant features.\n",
    "\n",
    "## References\n",
    "\n",
    "put a reference here\n",
    "\n",
    "All features are scaled.\n",
    "\n",
    "The Gisette dataset is often used to evaluate the performance of various machine learning algorithms, especially those designed for high-dimensional data. It provides a challenging testbed for algorithms due to its high dimensionality and the presence of irrelevant features.\n",
    "each line is like this: \n",
    "-1 1:-1 2:-1 3:0.913914 4:-1 5:-1 6:0.4530 ...\n",
    "the first number is either 1 or -1 (label y)\n",
    " and it is followed by 5000 pairs of the form integer_index. the floats are the x values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76d2d9a077d300a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.utils import read_gisette_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T13:53:30.971406Z",
     "start_time": "2024-06-21T13:53:30.821860Z"
    }
   },
   "id": "e40611c5cf56a43f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAX_LINES = 4500\n",
    "file_path_train = os.path.join(\"..\",\"data\",\"gisette_scale.bz2\")\n",
    "file_path_test = os.path.join(\"..\",\"data\",\"gisette_scale.t.bz2\")\n",
    "\n",
    "\n",
    "y_train, X_train = read_gisette_data(file_path_train, max_lines=MAX_LINES)\n",
    "y_test, X_test = read_gisette_data(file_path_test, max_lines=MAX_LINES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:09:06.768506Z",
     "start_time": "2024-06-21T15:08:56.343578Z"
    }
   },
   "id": "1325be0581428573",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM problem definition\n",
    "\n",
    "* the optimization problem we should solve it the following one:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\min \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i \\\\\n",
    "& \\text{subject to} \\quad y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\dots, m\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "According to Platt's algorithm [put reference here] it is preferrable to solve the dual, which is the following:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\max_{\\boldsymbol{\\alpha}} \\quad \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\cdot \\mathbf{x}_j \\\\\n",
    "& \\text{subject to} \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n",
    "& \\quad \\quad \\quad \\quad 0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\dots, m\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "     "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c5ce1ebe99c954"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(C=100, kernel='linear')",
      "text/html": "<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=100, kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(kernel='linear', C=100)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:09:15.025293Z",
     "start_time": "2024-06-21T15:09:06.769848Z"
    }
   },
   "id": "1fbaf86b4fe3cc56",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM on test set: 0.97\n",
      "F1 of SVM on test set: 0.97\n",
      "Precision of SVM on test set: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy of SVM on test set: {accuracy:.2f}')\n",
    "print(f'F1 of SVM on test set: {f1:.2f}')\n",
    "print(f'Precision of SVM on test set: {precision:.2f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:09:16.745223Z",
     "start_time": "2024-06-21T15:09:15.026753Z"
    }
   },
   "id": "90fe29e00b80b4bf",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom SMO algorithm\n",
    "\n",
    "* Create the SMO algorith\n",
    "* use classes in scikit-learn similar manner, so that has fit and predict methods for training and inference\n",
    "* the cor of the classifier class are the two functions described in Platt's paper `take_step` and `examine_example`\n",
    "* The main routine has been replaced with `fit` and has the two "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c7c80000d493dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SVM_classifier:\n",
    "    def __init__(self, X, y, kernel:str ='linear', C:float =1, epsilon:float = 1e-8, tol:float = 0.001, max_iter:int= 500):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.kernel = kernel\n",
    "        self.kernel_func = self.select_kernel(self.kernel)\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon # error margin \n",
    "        self.tol = tol # tolerance for KKT\n",
    "        self.max_iter = max_iter\n",
    "        self.m, self.n = np.shape(self.X) # m is number of samples, n number of features\n",
    "        \n",
    "        self.alphas = np.zeros(self.m)\n",
    "        self.Error_cache = - y \n",
    "        \n",
    "        # If the kernel is linear we can store a single weight vector and use the alternative implemented in SVM\n",
    "        \n",
    "        self.w = np.zeros(self.n)\n",
    "        self.b = 0 # intercept            \n",
    "        \n",
    "        \n",
    "    def select_kernel(self, kernel:str):\n",
    "        \n",
    "        ''' We have to choose a kernel based on the kernel type argument\n",
    "        here we can use only linear or the gaussion, no other kernels are available'''\n",
    "        \n",
    "        if kernel == 'linear':\n",
    "            return self.linear_kernel\n",
    "        elif kernel == 'rbf':\n",
    "            return self.rbf_kernel\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported kernel type: {kernel}\")\n",
    "    \n",
    "    def linear_kernel(self, x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "        return x1 @ x2.T\n",
    "    \n",
    "    def rbf_kernel(self, x1: np.ndarray, x2: np.ndarray) -> np.ndarray:\n",
    "        if x1.ndim == 1:\n",
    "            x1 = x1.reshape(1, -1)\n",
    "        if x2.ndim == 1:\n",
    "            x2 = x2.reshape(1, -1)\n",
    "\n",
    "        # Compute the squared Euclidean distance between each pair of points\n",
    "        squares = np.sum(x1*x1, axis=1).reshape(-1, 1) + np.sum(x2*x2, axis=1) - 2 * np.dot(x1, x2.T)\n",
    "\n",
    "        # Compute the Gaussian kernel with auto-scaling\n",
    "        gamma = 1.0/x1.size\n",
    "        K = np.exp(-gamma*squares)\n",
    "        return K  \n",
    "    \n",
    "    \n",
    "    def predict(self,x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Predicts the labels for the instance\n",
    "            and the respective score.\"\"\"\n",
    "\n",
    "        # if self.kernel != 'linear':\n",
    "        #     w = self.alphas * self.y\n",
    "        #     scores = w @ self.kernel_func(self.X, x) + self.b\n",
    "        # else:\n",
    "        #     scores = self.w @ self.kernel_func(self.X,x) + self.b\n",
    "        \n",
    "        scores = (self.alphas * self.y) @ self.kernel_func(self.X,x) - self.b\n",
    "        pred = np.sign(scores)\n",
    "\n",
    "        return pred, scores\n",
    "    \n",
    "    def take_step(self, i1:int=None, i2:int=None) -> int:\n",
    "        \"\"\"\n",
    "        takes one step of the SMO algorithm\n",
    "        :param i1: i1-th training instance \n",
    "        :param i2: i2-th training instance\n",
    "        :return: 1 if success else 0\n",
    "        \"\"\"\n",
    "        #print(\"I an in take_step.\")\n",
    "        if i1==i2:\n",
    "            #print(\"i1==i2\\n0 returned.\")\n",
    "            return 0\n",
    "        \n",
    "        # Set all required parameters\n",
    "        a1 = self.alphas[i1]\n",
    "        a2 = self.alphas[i2]\n",
    "        \n",
    "        x1 = self.X[i1,:]\n",
    "        x2 = self.X[i2,:]\n",
    "        \n",
    "        y1 = self.y[i1]\n",
    "        y2 = self.y[i2]\n",
    "        \n",
    "        E1 = self.Error_cache[i1] # HERE is the error\n",
    "        E2 = self.Error_cache[i2]\n",
    "        \n",
    "        # Define parameter s\n",
    "        s = y1*y2\n",
    "        \n",
    "        # Compute L, H via equations (13) and (14) from Platt\n",
    "        if y1!=y2:\n",
    "            L = max(0,a2-a1)\n",
    "            H = min(self.C,self.C+a2-a1)\n",
    "        else:\n",
    "            L = max(0,a2+a1-self.C)\n",
    "            H = min(self.C,a2+a1)\n",
    "            \n",
    "        if L==H:\n",
    "            #print(\"L==H\\n0 returned.\")\n",
    "            return 0\n",
    "        \n",
    "        k11 = self.kernel_func(x1,x1)\n",
    "        k22 = self.kernel_func(x2,x2)\n",
    "        k12 = self.kernel_func(x1,x2)\n",
    "        \n",
    "        # Compute the second derivative of the objective function along the diagonal line\n",
    "        eta = k11 + k22 - 2.0*k12\n",
    "        \n",
    "        if eta > 0:\n",
    "            # Normal circumstances, using Equations (16)-(18) to compute a1 and a2\n",
    "            a2_new = a2 +y2*(E1-E2)/eta\n",
    "            \n",
    "            if a2_new>=H:\n",
    "                a2_new = H\n",
    "            if a2_new<=L:\n",
    "                a2_new = L\n",
    "        else:\n",
    "            # Strange case, we use Equations (19)\n",
    "            f1 = y1*(E1 + self.b) - a1*k11 - s*a2*k12\n",
    "            f2 = y2*(E2 + self.b) - s*a1*k12 - a2*k22\n",
    "            L1 = a1 + s*(a2 - L)\n",
    "            H1 = a1 + s*(a2 - H)\n",
    "            psi_L = L1*f1 + L*f2 + 0.5*L1*L1*k11 + 0.5*L*L*k22 + s*L*L1*k12\n",
    "            psi_H = H1*f1 + H*f2 + 0.5*H1*H1*k11 + 0.5*H*H*k22 + s*H*H1*k12\n",
    "            \n",
    "            if psi_L < (psi_H - self.epsilon):\n",
    "                a2_new = L\n",
    "            elif psi_L > (psi_H + self.epsilon):\n",
    "                a2_new = H\n",
    "            else:\n",
    "                a2_new = a2\n",
    "\n",
    "        # if a2 very close to zero or C set a to 0 or C respectively\n",
    "        #if a2 < (10 ** (-8)):\n",
    "        #    a2 = 0.0\n",
    "        #elif a2 > self.C - (10**-8):\n",
    "        #    a2 = self.C\n",
    "        \n",
    "        if np.abs(a2_new - a2) < (self.epsilon * (a2_new + a2 + self.epsilon)):\n",
    "            #print(\"off numerical tolerance\\n0 returned.\")\n",
    "            return 0\n",
    "        \n",
    "        # Calculate a1_new\n",
    "        a1_new = a1 + s*(a2 - a2_new)\n",
    "        \n",
    "        # Push alphas to boundaries\n",
    "        if a1_new < self.epsilon:\n",
    "            a1_new = 0\n",
    "        if a1_new > (self.C - self.epsilon):\n",
    "            a1_new = self.C\n",
    "        \n",
    "        # Update threshold b\n",
    "        b1 = self.b + E1 + y1*(a1_new - a1)*k11 + y2*(a2_new - a2)*k12\n",
    "        b2 = self.b + E2 + y1*(a1_new - a1)*k12 + y2*(a2_new - a2)*k22\n",
    "        \n",
    "        if 0 < a1_new < self.C:\n",
    "            b_new = b1\n",
    "        elif 0 < a2_new < self.C:\n",
    "            b_new = b2\n",
    "        else:\n",
    "            b_new = 0.5*(b1 + b2)\n",
    "            \n",
    "        # Update weight's vector if Linear kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.w = self.w + y1*(a1_new - a1)*x1 + y2*(a2_new - a2)*x2\n",
    "            \n",
    "        # Update Error_cache using alphas (see reference)\n",
    "        \n",
    "        # if a1 & a2 are not at bounds, the error will be 0\n",
    "        self.Error_cache[i1] = 0\n",
    "        self.Error_cache[i2] = 0\n",
    "        \n",
    "        # Update error for non boundary elements\n",
    "        inner_indices = [idx for idx, a in enumerate(self.alphas) if 0 < a < self.C]\n",
    "        for i in inner_indices:\n",
    "            self.Error_cache[i] += \\\n",
    "                 y1*(a1_new - a1)*self.kernel_func(x1, self.X[i,:]) \\\n",
    "               + y2*(a2_new - a2)*self.kernel_func(x2, self.X[i,:]) \\\n",
    "               + (self.b - b_new)\n",
    "        \n",
    "        \n",
    "        # Update alphas\n",
    "        self.alphas[i1] = a1_new\n",
    "        self.alphas[i2] = a2_new\n",
    "        \n",
    "        # Update b\n",
    "        self.b = b_new\n",
    "        \n",
    "        #print(\"successfull pass\")\n",
    "        return 1 # sucessfull pass\n",
    "    \n",
    "    def examine_example(self, i2:int=None):\n",
    "        \"\"\"\n",
    "        Examine the i2-th example in the algorithm to determine\n",
    "        if eligible for usage in optimization pair\n",
    "        :param i2: example to examine\n",
    "        :return: 1 if successful, 0 if not\n",
    "        \"\"\"\n",
    "        \n",
    "        y2 = self.y[i2]\n",
    "        a2 = self.alphas[i2]\n",
    "        E2 = self.Error_cache[i2]\n",
    "        r2 = E2 * y2\n",
    "        #print(\"I am in examine_example()\")\n",
    "        # Check if error is within tolerance\n",
    "        if (r2 < -self.tol and a2 < self.C) or (r2 > self.tol and a2 > 0):\n",
    "            #print(\"Error within tolerance.\")\n",
    "            # If there are more than one non-bound elements use the second heuristic \n",
    "            if np.count_nonzero((0 < self.alphas) & (self.alphas < self.C)) > 1:\n",
    "                \n",
    "                # use section 2.2 to select i1\n",
    "                if E2 > 0:\n",
    "                    i1 = np.argmin(self.Error_cache)\n",
    "                else:\n",
    "                    i1 = np.argmax(self.Error_cache)\n",
    "\n",
    "                if self.take_step(i1, i2):\n",
    "                    return 1\n",
    "                \n",
    "            # Loop over all non-zero and non-C alpha, starting at a random point\n",
    "\n",
    "            # Get indices where 0 < alpha < self.C\n",
    "            i1_array = np.where((0 < self.alphas) & (self.alphas < self.C))[0]\n",
    "\n",
    "            # Roll the array by a random number of positions to ensure that we will pass all\n",
    "            random_shift = np.random.choice(np.arange(self.m))\n",
    "            i1_list = np.roll(i1_array, random_shift)\n",
    "            \n",
    "            # Loop over all non-boundary elements\n",
    "            for i1 in i1_list:\n",
    "                if self.take_step(i1, i2):\n",
    "                    return 1\n",
    "\n",
    "            # Loop over all possible alpha elements, starting at a random point\n",
    "            i1_list = np.roll(np.arange(self.m), np.random.choice(np.arange(self.m)))\n",
    "            for i1 in i1_list:\n",
    "                if self.take_step(i1, i2):\n",
    "                    return 1\n",
    "                \n",
    "                    \n",
    "        return 0\n",
    "    \n",
    "    def fit(self) -> None:\n",
    "        \"\"\"This is the equivalent of the main routine in the original SMO paper.\n",
    "            We use it for training the algorithm.\"\"\"\n",
    "        iteration_number = 0 # We count the number of iterations and bounded below max_iter\n",
    "        numbers_changed = 0\n",
    "        examine_all = True\n",
    "    \n",
    "        while numbers_changed > 0 or examine_all:\n",
    "            \n",
    "            if iteration_number >= self.max_iter:\n",
    "                break\n",
    "                \n",
    "            numbers_changed = 0\n",
    "            if examine_all:\n",
    "                # Loop i over all training examples\n",
    "                for i in range(self.m):\n",
    "                    numbers_changed += self.examine_example(i)\n",
    "            \n",
    "            else: \n",
    "                # Loop i over examples where alpha is not 0 & not C\n",
    "                i_array = np.where((0 < self.alphas) & (self.alphas < self.C))[0]\n",
    "                for i in i_array:\n",
    "                    numbers_changed += self.examine_example(i)\n",
    "                    \n",
    "            if examine_all:\n",
    "                examine_all = False\n",
    "            if numbers_changed == 0:\n",
    "                examine_all = True\n",
    "                \n",
    "            iteration_number += 1\n",
    "            \n",
    "            #print(f\"Iteration:{iteration_number} ended.\")\n",
    "            #return self.b, self.w\n",
    "        \n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:26:06.333872Z",
     "start_time": "2024-06-21T15:26:06.317735Z"
    }
   },
   "id": "fceae32f557ce1c1",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1 ended.\n",
      "Iteration:2 ended.\n",
      "Iteration:3 ended.\n",
      "Iteration:4 ended.\n",
      "Iteration:5 ended.\n",
      "Iteration:6 ended.\n",
      "Iteration:7 ended.\n",
      "Iteration:8 ended.\n",
      "Iteration:9 ended.\n",
      "Iteration:10 ended.\n",
      "Iteration:11 ended.\n",
      "Iteration:12 ended.\n",
      "Iteration:13 ended.\n",
      "Iteration:14 ended.\n",
      "Iteration:15 ended.\n",
      "Iteration:16 ended.\n",
      "Iteration:17 ended.\n",
      "Iteration:18 ended.\n",
      "Iteration:19 ended.\n",
      "Iteration:20 ended.\n",
      "Iteration:21 ended.\n",
      "Iteration:22 ended.\n",
      "Iteration:23 ended.\n",
      "Iteration:24 ended.\n",
      "Iteration:25 ended.\n",
      "Iteration:26 ended.\n",
      "Iteration:27 ended.\n",
      "Iteration:28 ended.\n",
      "Iteration:29 ended.\n",
      "Iteration:30 ended.\n",
      "Iteration:31 ended.\n",
      "Iteration:32 ended.\n",
      "Iteration:33 ended.\n",
      "Iteration:34 ended.\n",
      "Iteration:35 ended.\n",
      "Iteration:36 ended.\n",
      "Iteration:37 ended.\n",
      "Iteration:38 ended.\n",
      "Iteration:39 ended.\n",
      "Iteration:40 ended.\n",
      "Iteration:41 ended.\n",
      "Iteration:42 ended.\n",
      "Iteration:43 ended.\n",
      "Iteration:44 ended.\n",
      "Iteration:45 ended.\n",
      "Iteration:46 ended.\n",
      "Iteration:47 ended.\n",
      "Iteration:48 ended.\n",
      "Iteration:49 ended.\n",
      "Iteration:50 ended.\n",
      "Iteration:51 ended.\n",
      "Iteration:52 ended.\n",
      "Iteration:53 ended.\n",
      "Iteration:54 ended.\n",
      "Iteration:55 ended.\n",
      "Iteration:56 ended.\n",
      "Iteration:57 ended.\n",
      "Iteration:58 ended.\n",
      "Iteration:59 ended.\n",
      "Iteration:60 ended.\n",
      "Iteration:61 ended.\n",
      "Iteration:62 ended.\n",
      "Iteration:63 ended.\n",
      "Iteration:64 ended.\n",
      "Iteration:65 ended.\n",
      "Iteration:66 ended.\n",
      "Iteration:67 ended.\n",
      "Iteration:68 ended.\n",
      "Iteration:69 ended.\n",
      "Iteration:70 ended.\n",
      "Iteration:71 ended.\n",
      "Iteration:72 ended.\n",
      "Iteration:73 ended.\n",
      "Iteration:74 ended.\n",
      "Iteration:75 ended.\n",
      "Iteration:76 ended.\n",
      "Iteration:77 ended.\n",
      "Iteration:78 ended.\n",
      "Iteration:79 ended.\n",
      "Iteration:80 ended.\n",
      "Iteration:81 ended.\n",
      "Iteration:82 ended.\n",
      "Iteration:83 ended.\n",
      "Iteration:84 ended.\n",
      "Iteration:85 ended.\n",
      "Iteration:86 ended.\n",
      "Iteration:87 ended.\n",
      "Iteration:88 ended.\n",
      "Iteration:89 ended.\n",
      "Iteration:90 ended.\n",
      "Iteration:91 ended.\n",
      "Iteration:92 ended.\n",
      "Iteration:93 ended.\n",
      "Iteration:94 ended.\n",
      "Iteration:95 ended.\n",
      "Iteration:96 ended.\n",
      "Iteration:97 ended.\n",
      "Iteration:98 ended.\n",
      "Iteration:99 ended.\n",
      "Iteration:100 ended.\n",
      "Iteration:101 ended.\n",
      "Iteration:102 ended.\n",
      "Iteration:103 ended.\n",
      "Iteration:104 ended.\n",
      "Iteration:105 ended.\n",
      "Iteration:106 ended.\n",
      "Iteration:107 ended.\n",
      "Iteration:108 ended.\n",
      "Iteration:109 ended.\n",
      "Iteration:110 ended.\n",
      "Iteration:111 ended.\n",
      "Iteration:112 ended.\n",
      "Iteration:113 ended.\n",
      "Iteration:114 ended.\n",
      "Iteration:115 ended.\n",
      "Iteration:116 ended.\n",
      "Iteration:117 ended.\n",
      "Iteration:118 ended.\n",
      "Iteration:119 ended.\n",
      "Iteration:120 ended.\n",
      "Iteration:121 ended.\n",
      "Iteration:122 ended.\n",
      "Iteration:123 ended.\n",
      "Iteration:124 ended.\n",
      "Iteration:125 ended.\n",
      "Iteration:126 ended.\n",
      "Iteration:127 ended.\n",
      "Iteration:128 ended.\n",
      "Iteration:129 ended.\n",
      "Iteration:130 ended.\n",
      "Iteration:131 ended.\n",
      "Iteration:132 ended.\n",
      "Iteration:133 ended.\n",
      "Iteration:134 ended.\n",
      "Iteration:135 ended.\n",
      "Iteration:136 ended.\n",
      "Iteration:137 ended.\n",
      "Iteration:138 ended.\n",
      "Iteration:139 ended.\n",
      "Iteration:140 ended.\n",
      "Iteration:141 ended.\n",
      "Iteration:142 ended.\n",
      "Iteration:143 ended.\n",
      "Iteration:144 ended.\n",
      "Iteration:145 ended.\n",
      "Iteration:146 ended.\n",
      "Iteration:147 ended.\n",
      "Iteration:148 ended.\n",
      "Iteration:149 ended.\n",
      "Iteration:150 ended.\n",
      "Iteration:151 ended.\n",
      "Iteration:152 ended.\n",
      "Iteration:153 ended.\n",
      "Iteration:154 ended.\n",
      "Iteration:155 ended.\n",
      "Iteration:156 ended.\n",
      "Iteration:157 ended.\n",
      "Iteration:158 ended.\n",
      "Iteration:159 ended.\n",
      "Iteration:160 ended.\n",
      "Iteration:161 ended.\n",
      "Iteration:162 ended.\n",
      "Iteration:163 ended.\n",
      "Iteration:164 ended.\n",
      "Iteration:165 ended.\n",
      "Iteration:166 ended.\n",
      "Iteration:167 ended.\n",
      "Iteration:168 ended.\n",
      "Iteration:169 ended.\n",
      "Iteration:170 ended.\n",
      "Iteration:171 ended.\n",
      "Iteration:172 ended.\n",
      "Iteration:173 ended.\n",
      "Iteration:174 ended.\n",
      "Iteration:175 ended.\n",
      "Iteration:176 ended.\n",
      "Iteration:177 ended.\n",
      "Iteration:178 ended.\n",
      "Iteration:179 ended.\n",
      "Iteration:180 ended.\n",
      "Iteration:181 ended.\n",
      "Iteration:182 ended.\n",
      "Iteration:183 ended.\n",
      "Iteration:184 ended.\n",
      "Iteration:185 ended.\n",
      "Iteration:186 ended.\n",
      "Iteration:187 ended.\n",
      "Iteration:188 ended.\n",
      "Iteration:189 ended.\n",
      "Iteration:190 ended.\n",
      "Iteration:191 ended.\n",
      "Iteration:192 ended.\n",
      "Iteration:193 ended.\n",
      "Iteration:194 ended.\n",
      "Iteration:195 ended.\n",
      "Iteration:196 ended.\n",
      "Iteration:197 ended.\n",
      "Iteration:198 ended.\n",
      "Iteration:199 ended.\n",
      "Iteration:200 ended.\n",
      "Iteration:201 ended.\n",
      "Iteration:202 ended.\n",
      "Iteration:203 ended.\n",
      "Iteration:204 ended.\n",
      "Iteration:205 ended.\n",
      "Iteration:206 ended.\n",
      "Iteration:207 ended.\n",
      "Iteration:208 ended.\n",
      "Iteration:209 ended.\n",
      "Iteration:210 ended.\n",
      "Iteration:211 ended.\n",
      "Iteration:212 ended.\n",
      "Iteration:213 ended.\n",
      "Iteration:214 ended.\n",
      "Iteration:215 ended.\n",
      "Iteration:216 ended.\n",
      "Iteration:217 ended.\n",
      "Iteration:218 ended.\n",
      "Iteration:219 ended.\n",
      "Iteration:220 ended.\n",
      "Iteration:221 ended.\n",
      "Iteration:222 ended.\n",
      "Iteration:223 ended.\n",
      "Iteration:224 ended.\n",
      "Iteration:225 ended.\n",
      "Iteration:226 ended.\n",
      "Iteration:227 ended.\n",
      "Iteration:228 ended.\n",
      "Iteration:229 ended.\n",
      "Iteration:230 ended.\n",
      "Iteration:231 ended.\n",
      "Iteration:232 ended.\n",
      "Iteration:233 ended.\n",
      "Iteration:234 ended.\n",
      "Iteration:235 ended.\n",
      "Iteration:236 ended.\n",
      "Iteration:237 ended.\n",
      "Iteration:238 ended.\n",
      "Iteration:239 ended.\n",
      "Iteration:240 ended.\n",
      "Iteration:241 ended.\n",
      "Iteration:242 ended.\n",
      "Iteration:243 ended.\n",
      "Iteration:244 ended.\n",
      "Iteration:245 ended.\n",
      "Iteration:246 ended.\n",
      "Iteration:247 ended.\n",
      "Iteration:248 ended.\n",
      "Iteration:249 ended.\n",
      "Iteration:250 ended.\n",
      "Iteration:251 ended.\n",
      "Iteration:252 ended.\n",
      "Iteration:253 ended.\n",
      "Iteration:254 ended.\n",
      "Iteration:255 ended.\n",
      "Iteration:256 ended.\n",
      "Iteration:257 ended.\n",
      "Iteration:258 ended.\n",
      "Iteration:259 ended.\n",
      "Iteration:260 ended.\n",
      "Iteration:261 ended.\n",
      "Iteration:262 ended.\n",
      "Iteration:263 ended.\n",
      "Iteration:264 ended.\n",
      "Iteration:265 ended.\n",
      "Iteration:266 ended.\n",
      "Iteration:267 ended.\n",
      "Iteration:268 ended.\n",
      "Iteration:269 ended.\n",
      "Iteration:270 ended.\n",
      "Iteration:271 ended.\n",
      "Iteration:272 ended.\n",
      "Iteration:273 ended.\n",
      "Iteration:274 ended.\n",
      "Iteration:275 ended.\n",
      "Iteration:276 ended.\n",
      "Iteration:277 ended.\n",
      "Iteration:278 ended.\n",
      "Iteration:279 ended.\n",
      "Iteration:280 ended.\n",
      "Iteration:281 ended.\n",
      "Iteration:282 ended.\n",
      "Iteration:283 ended.\n",
      "Iteration:284 ended.\n",
      "Iteration:285 ended.\n",
      "Iteration:286 ended.\n",
      "Iteration:287 ended.\n",
      "Iteration:288 ended.\n",
      "Iteration:289 ended.\n",
      "Iteration:290 ended.\n",
      "Iteration:291 ended.\n",
      "Iteration:292 ended.\n",
      "Iteration:293 ended.\n",
      "Iteration:294 ended.\n",
      "Iteration:295 ended.\n",
      "Iteration:296 ended.\n",
      "Iteration:297 ended.\n",
      "Iteration:298 ended.\n",
      "Iteration:299 ended.\n",
      "Iteration:300 ended.\n",
      "Iteration:301 ended.\n",
      "Iteration:302 ended.\n",
      "Iteration:303 ended.\n",
      "Iteration:304 ended.\n",
      "Iteration:305 ended.\n",
      "Iteration:306 ended.\n",
      "Iteration:307 ended.\n",
      "Iteration:308 ended.\n",
      "Iteration:309 ended.\n",
      "Iteration:310 ended.\n",
      "Iteration:311 ended.\n",
      "Iteration:312 ended.\n",
      "Iteration:313 ended.\n",
      "Iteration:314 ended.\n",
      "Iteration:315 ended.\n",
      "Iteration:316 ended.\n",
      "Iteration:317 ended.\n",
      "Iteration:318 ended.\n",
      "Iteration:319 ended.\n",
      "Iteration:320 ended.\n",
      "Iteration:321 ended.\n",
      "Iteration:322 ended.\n",
      "Iteration:323 ended.\n",
      "Iteration:324 ended.\n",
      "Iteration:325 ended.\n",
      "Iteration:326 ended.\n",
      "Iteration:327 ended.\n",
      "Iteration:328 ended.\n",
      "Iteration:329 ended.\n",
      "Iteration:330 ended.\n",
      "Iteration:331 ended.\n",
      "Iteration:332 ended.\n",
      "Iteration:333 ended.\n",
      "Iteration:334 ended.\n",
      "Iteration:335 ended.\n",
      "Iteration:336 ended.\n",
      "Iteration:337 ended.\n",
      "Iteration:338 ended.\n",
      "Iteration:339 ended.\n",
      "Iteration:340 ended.\n",
      "Iteration:341 ended.\n",
      "Iteration:342 ended.\n",
      "Iteration:343 ended.\n",
      "Iteration:344 ended.\n",
      "Iteration:345 ended.\n",
      "Iteration:346 ended.\n",
      "Iteration:347 ended.\n",
      "Iteration:348 ended.\n",
      "Iteration:349 ended.\n",
      "Iteration:350 ended.\n",
      "Iteration:351 ended.\n",
      "Iteration:352 ended.\n",
      "Iteration:353 ended.\n",
      "Iteration:354 ended.\n",
      "Iteration:355 ended.\n",
      "Iteration:356 ended.\n",
      "Iteration:357 ended.\n",
      "Iteration:358 ended.\n",
      "Iteration:359 ended.\n",
      "Iteration:360 ended.\n",
      "Iteration:361 ended.\n",
      "Iteration:362 ended.\n",
      "Iteration:363 ended.\n",
      "Iteration:364 ended.\n",
      "Iteration:365 ended.\n",
      "Iteration:366 ended.\n",
      "Iteration:367 ended.\n",
      "Iteration:368 ended.\n",
      "Iteration:369 ended.\n",
      "Iteration:370 ended.\n",
      "Iteration:371 ended.\n",
      "Iteration:372 ended.\n",
      "Iteration:373 ended.\n",
      "Iteration:374 ended.\n",
      "Iteration:375 ended.\n",
      "Iteration:376 ended.\n",
      "Iteration:377 ended.\n",
      "Iteration:378 ended.\n",
      "Iteration:379 ended.\n",
      "Iteration:380 ended.\n",
      "Iteration:381 ended.\n",
      "Iteration:382 ended.\n",
      "Iteration:383 ended.\n",
      "Iteration:384 ended.\n",
      "Iteration:385 ended.\n",
      "Iteration:386 ended.\n",
      "Iteration:387 ended.\n",
      "Iteration:388 ended.\n",
      "Iteration:389 ended.\n",
      "Iteration:390 ended.\n",
      "Iteration:391 ended.\n",
      "Iteration:392 ended.\n",
      "Iteration:393 ended.\n",
      "Iteration:394 ended.\n",
      "Iteration:395 ended.\n",
      "Iteration:396 ended.\n",
      "Iteration:397 ended.\n",
      "Iteration:398 ended.\n",
      "Iteration:399 ended.\n",
      "Iteration:400 ended.\n",
      "Iteration:401 ended.\n",
      "Iteration:402 ended.\n",
      "Iteration:403 ended.\n",
      "Iteration:404 ended.\n",
      "Iteration:405 ended.\n",
      "Iteration:406 ended.\n",
      "Iteration:407 ended.\n",
      "Iteration:408 ended.\n",
      "Iteration:409 ended.\n",
      "Iteration:410 ended.\n",
      "Iteration:411 ended.\n",
      "Iteration:412 ended.\n",
      "Iteration:413 ended.\n",
      "Iteration:414 ended.\n",
      "Iteration:415 ended.\n",
      "Iteration:416 ended.\n",
      "Iteration:417 ended.\n",
      "Iteration:418 ended.\n",
      "Iteration:419 ended.\n",
      "Iteration:420 ended.\n",
      "Iteration:421 ended.\n",
      "Iteration:422 ended.\n",
      "Iteration:423 ended.\n",
      "Iteration:424 ended.\n",
      "Iteration:425 ended.\n",
      "Iteration:426 ended.\n",
      "Iteration:427 ended.\n",
      "Iteration:428 ended.\n",
      "Iteration:429 ended.\n",
      "Iteration:430 ended.\n",
      "Iteration:431 ended.\n",
      "Iteration:432 ended.\n",
      "Iteration:433 ended.\n",
      "Iteration:434 ended.\n",
      "Iteration:435 ended.\n",
      "Iteration:436 ended.\n",
      "Iteration:437 ended.\n",
      "Iteration:438 ended.\n",
      "Iteration:439 ended.\n",
      "Iteration:440 ended.\n",
      "Iteration:441 ended.\n",
      "Iteration:442 ended.\n",
      "Iteration:443 ended.\n",
      "Iteration:444 ended.\n",
      "Iteration:445 ended.\n",
      "Iteration:446 ended.\n",
      "Iteration:447 ended.\n",
      "Iteration:448 ended.\n",
      "Iteration:449 ended.\n",
      "Iteration:450 ended.\n",
      "Iteration:451 ended.\n",
      "Iteration:452 ended.\n",
      "Iteration:453 ended.\n",
      "Iteration:454 ended.\n",
      "Iteration:455 ended.\n",
      "Iteration:456 ended.\n",
      "Iteration:457 ended.\n",
      "Iteration:458 ended.\n",
      "Iteration:459 ended.\n",
      "Iteration:460 ended.\n",
      "Iteration:461 ended.\n",
      "Iteration:462 ended.\n",
      "Iteration:463 ended.\n",
      "Iteration:464 ended.\n",
      "Iteration:465 ended.\n",
      "Iteration:466 ended.\n",
      "Iteration:467 ended.\n",
      "Iteration:468 ended.\n",
      "Iteration:469 ended.\n",
      "Iteration:470 ended.\n",
      "Iteration:471 ended.\n",
      "Iteration:472 ended.\n",
      "Iteration:473 ended.\n",
      "Iteration:474 ended.\n",
      "Iteration:475 ended.\n",
      "Iteration:476 ended.\n",
      "Iteration:477 ended.\n",
      "Iteration:478 ended.\n",
      "Iteration:479 ended.\n",
      "Iteration:480 ended.\n",
      "Iteration:481 ended.\n",
      "Iteration:482 ended.\n",
      "Iteration:483 ended.\n",
      "Iteration:484 ended.\n",
      "Iteration:485 ended.\n",
      "Iteration:486 ended.\n",
      "Iteration:487 ended.\n",
      "Iteration:488 ended.\n",
      "Iteration:489 ended.\n",
      "Iteration:490 ended.\n",
      "Iteration:491 ended.\n",
      "Iteration:492 ended.\n",
      "Iteration:493 ended.\n",
      "Iteration:494 ended.\n",
      "Iteration:495 ended.\n",
      "Iteration:496 ended.\n",
      "Iteration:497 ended.\n",
      "Iteration:498 ended.\n",
      "Iteration:499 ended.\n",
      "Iteration:500 ended.\n"
     ]
    }
   ],
   "source": [
    "model = SVM_classifier(X_train, y_train, kernel='linear', C=10)\n",
    "model.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:34.419798Z",
     "start_time": "2024-06-21T15:28:05.982334Z"
    }
   },
   "id": "c14d232879fe68b1",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM on test set: 0.79\n",
      "F1 of SVM on test set: 0.82\n",
      "Precision of SVM on test set: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred, scores = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy of SVM on test set: {accuracy:.2f}')\n",
    "print(f'F1 of SVM on test set: {f1:.2f}')\n",
    "print(f'Precision of SVM on test set: {precision:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:28:38.754167Z",
     "start_time": "2024-06-21T15:28:38.489075Z"
    }
   },
   "id": "33c1b9d9c4b63256",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "500"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_iter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:13:15.522880Z",
     "start_time": "2024-06-21T15:13:15.518766Z"
    }
   },
   "id": "9e478eca7128b3d5",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.01302314, -0.00079364,  0.00079364, ...,  0.00074602,\n        0.00097505,  0.00081396])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.alphas * model.y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:13:17.109631Z",
     "start_time": "2024-06-21T15:13:17.104546Z"
    }
   },
   "id": "137150d1a5b964d1",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1, -1,  1, ..., -1, -1, -1])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Error_cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T10:15:59.678255Z",
     "start_time": "2024-06-21T10:15:59.673757Z"
    }
   },
   "id": "7699e07c548713e4",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6000 is different from 5000)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m model\u001B[38;5;241m.\u001B[39mb\n",
      "\u001B[0;31mValueError\u001B[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6000 is different from 5000)"
     ]
    }
   ],
   "source": [
    "model.w @ model.kernel_func(model.X,X_test) + model.b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T09:58:43.194825Z",
     "start_time": "2024-06-21T09:58:42.804680Z"
    }
   },
   "id": "34778774756e0fc6",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 154.60336729,  312.04830122,  145.00765438,  278.12430417,\n        299.48779679,   71.26228702, -102.98908487,   98.01602573,\n         41.54913322,   71.68895929,  254.6402256 ,  229.69593321,\n        188.91696722,  284.84625698,   52.21427784,  249.39773982,\n        299.09237365,  280.495324  ,  284.89015868,  314.441512  ,\n         30.75190122,  315.71076547,  292.14330305,  244.86023937,\n        227.01679604,   94.91369686,  298.25585023,  313.92748312,\n        216.60662911,  233.44074888,  193.99826832,  253.700384  ,\n        137.95949434,  272.28310868,  243.29085382,  307.21791766,\n         87.6843977 ,  300.96957551,   49.00330707, -146.05886609,\n        291.53471036,   60.45145777,  272.68651169,  295.94075274,\n        -40.93146466, -138.76999412,  270.67131207,  169.05336082,\n        175.41469554,  208.69377914,  245.63514145,   83.79987473,\n         77.84801185,   34.77912159,  236.93496586,  311.91305272,\n         68.04736882,  -12.36721478,  217.54270247,  282.33363102,\n        261.131583  ,  132.28830515,  -57.51956637,  311.58348726,\n         53.52178147,   81.28315437,  292.98224963,  295.89271394,\n        334.50207865,  294.34948235,  223.63933298,  146.61915811,\n        173.819396  ,  325.2647581 ,  243.62165305,  298.66479394,\n        198.40068805,  -78.25528053,  288.28637202,  324.22626062,\n       -143.59566118,  327.16066084,  170.26999173,  242.44659415,\n        231.65436336,  214.2198015 ,   14.06052806,  325.66101082,\n        198.19710352,  282.93382853,  262.24350404,  209.34340296,\n        175.46974676,   15.30781198,  289.51770355,  151.27536021,\n        311.21032676,   78.87153357,  129.08489787,  324.67090812,\n        138.63648971,   77.39355315,  227.75272332,  333.53538511,\n        313.20813983,  241.79305694,   27.72985337,  288.66954331,\n        258.48883196,   35.64807465,  179.8484418 ,   64.38795622,\n        205.88018771,  135.28847861,  224.3589413 ,  328.8222928 ,\n         -1.50523944,    3.67906435,  118.21276926,  187.6224317 ,\n        129.57928765,  299.38270475,    3.82247574,  221.17230355,\n         84.7524263 ,   41.96135365,  344.66872139,   -2.59708108,\n         41.62160694,  -33.37223114,  207.38557733,  136.66614147,\n         96.57295158,  225.31803603,   91.16515712,  225.58184851,\n        214.70046655,  321.02680256,  133.38602545,  216.90584444,\n        271.02647565,  243.99514172,   34.23780389,  279.24154507,\n        232.2885084 ,  168.86897336,  149.79540715,  263.36864841,\n        349.49907419,    3.41785427,  271.35952176, -119.2252457 ,\n        254.31321801,  289.70136327,  217.44665701,  121.75596911,\n        259.72042529,  -13.24977554,  282.83242653,  167.99144014,\n        288.35814395,   98.29938619,  307.36046477,   99.07410685,\n         99.86448442,    9.02110832,   -4.71722019,  -36.78096671,\n        348.70472409,  305.50303336,  233.41603597,  194.64961666,\n        -61.53634407,  188.75061698,  273.58441587,  249.15782486,\n        174.21163945,   22.53038391,  259.85568293,   99.55624508,\n       -110.40917722,  215.7809702 ,   93.45439033,  154.24454085,\n         48.87139323,  287.24190175,  229.74909798,  276.7447603 ,\n        287.98693817,  117.52426481,  318.75978426,  100.77046983,\n        339.62022476, -161.90626908,  284.56766527,   88.75431522,\n         97.59068715,   26.16363034,  266.87063991,  326.33439046,\n        175.87107692,  249.93057406,   29.74775539,  254.92149145,\n        -84.21990258,  146.22459562,  315.4097273 ,   36.23635987,\n        336.91875114,  105.95964751,  235.78141594,   92.3296469 ,\n        316.13474716,  289.28781607, -164.72982976,  154.69720045,\n        177.66755533, -115.11948458,  295.27341071,  -26.8026778 ,\n        156.22229405,  180.23721514,  189.37538825,  281.17625247,\n         58.45568827,  121.70930255,  350.71984424,  267.37415169,\n        128.84601401,   40.31678497,  -39.29300644,  -18.37911604,\n        256.15627396,  -52.374134  ,  166.74045402,   72.03607408,\n       -114.62584186,  283.75280699,  131.37844686,  200.07031162,\n        244.60911925,  236.7590758 ,   69.33499823,  -54.74512992,\n       -121.79430452,  113.3351464 ,   92.57102466,   97.04424948,\n        256.90429524,  250.46771548,  271.58268813,  292.69493756,\n        239.31124282,   68.07278649,  303.8952597 ,  132.63096597,\n        249.79315061,  244.93584461,  107.25452393,  271.17338964,\n        333.95367258,  195.44708658,  106.35508531,  108.24168579,\n        155.04142959,  279.64939087,  130.67904833,  247.05967239,\n        228.1298145 ,  188.58115977,   89.71233351,  -10.68057866,\n        189.72536851,  232.46874198,  -33.75054003,   43.85028625,\n         52.4382106 ,  225.87985865,  328.72048246,  217.40198532,\n        283.04326827,  234.58927706,   78.4265773 ,  322.9830152 ,\n         41.67887262,   71.41367566,  162.58703278,  205.79502116,\n         21.63958678,  289.28956932,  -59.81199364,  -62.67175493,\n        176.76876536,  195.48117068,  -84.28402843,  291.15006575,\n        237.5974627 ,  218.72341402,  284.59637453,  178.45478888,\n        230.92674132,  308.0332659 ,   44.49762998,   91.18575198,\n        -79.05220045,  173.21141402,  128.94053277,  193.58433386,\n         59.05673435,  146.83483296,  357.03954927,  297.78468931,\n         45.51592783,  -21.07725984,  174.9261379 ,  309.2800377 ,\n         91.87910102,   68.46207682,  327.92469865,  126.58142589,\n       -117.32926904,  198.17438807,  246.30733309,  -72.22489821,\n        183.89358302,  -97.46130206,  173.45682931,  331.93733873,\n        178.94131103,  158.89689955,  165.8047073 , -161.68005212,\n         96.91228979,   54.90128697,   50.93601522,  323.69242446,\n        352.57807235,  214.06269206,  242.19956538,  312.99478221,\n        216.54040016,  317.79403563,  155.15317724,  127.17710109,\n        -68.37958587,  216.86164364,  301.74995336,  253.87237783,\n        264.66656692,  -79.14421303,  213.0825008 ,  150.08268892,\n        203.26850606,  150.01619287,  -73.21961953,  180.46392304,\n        310.57073209,  247.43747941,  269.30682679,  155.12183861,\n        245.86623927,  291.88377813,  318.22988023,  -37.47767038,\n        164.40265498,  112.07107585,  153.21485585,  279.53002054,\n         91.42790939,  246.7746496 ,  150.94461027,  -62.06618671,\n        249.75295816,  261.17675952,   19.64441017,  -84.27490523,\n        -24.95270419,  304.67492256,  310.1589812 ,  135.15444229,\n        259.75427429,  253.47160549,  327.13037555,   61.82392382,\n        220.29096621,  125.86731712,  -45.64757341,  245.03884281,\n        301.40756871,  185.8006273 ,   35.10694564,  245.83473556,\n        274.47937778,  129.24401391,   14.19221106,  152.52307703,\n        321.90657675, -105.12045432,  281.99153968,  287.18371429,\n        301.25930215,  381.3625576 ,  105.84567469,  205.08605162,\n        248.43775124,  190.02986921,   64.75153964,  208.21410524,\n        146.84763397,  186.8872843 ,  -48.1819143 ,  267.46180956,\n        150.51729533,   86.50785264,  215.83524046,  316.39131139,\n        198.02615537, -101.30661667,   84.34700539, -164.72000084,\n        306.93889228,  203.65127809,  315.54717872,   35.31755548,\n        277.02923865,   64.58207581,  313.19904484,  364.17361382,\n        -21.93271086,  158.36067939,  234.77928828,  129.87890366,\n        117.12887034,  138.73637916, -155.00393032,  159.80139339,\n        315.78803069,  323.40250312,  334.80049632,  232.89759611,\n        135.10846565,  132.13926579,  -42.84543262,  295.61099892,\n        308.62322628,  272.20346383,  134.33154204,  -21.62142763,\n        120.23192885,   -7.44777687,  178.36027161,  255.91665935,\n         47.28416321,  165.04755413,   74.42698894,  201.3058161 ,\n        174.46722819,    9.5756693 ,  270.95951233,  126.85440778,\n        -13.13612647,  203.24065088,   90.62892259,  129.64421085,\n        286.664655  ,  286.89121996,  307.77958401,   27.96315187,\n        278.43747001,  -27.02977604,  251.8437887 ,   18.92665811,\n        -64.59768372,  228.43252742,  272.50474714,  251.95663247,\n        252.95686129,  122.30959373,  128.70897273,  243.13648079,\n        293.5532542 ,  290.72381695,  195.02496907,  101.25439401,\n        338.22248861,  256.42073042,  142.70378357,  201.84641425,\n        309.8740033 ,   79.95948773,   61.73976658,  202.8452085 ,\n        260.0430514 ,  115.9134699 ,  102.47300502,  290.06276539,\n        165.94699475,  210.61546911,  250.28039568,  131.94014098,\n        257.85194101,  308.7941713 ,   25.11584196,  -39.0035383 ,\n        129.49131339, -166.21965894,  139.37359628,  169.00633394,\n        219.35028733,  220.53228628,  148.64974787,  249.24943674,\n        178.52567725,   93.39575521,   56.72938789,  122.57357324,\n        160.96502291,   72.71750576,   -5.21040584,   32.97602532,\n        171.36571977, -115.46114809,  207.54767014,  229.90291008,\n        139.54789928,   36.33714716,  193.02206173,  283.29073627,\n        207.61365167,  283.09815025,  189.31055004,  -16.61882333,\n        228.97442093,   70.94587223,   38.67602228,  218.08189604,\n        171.8280866 ,  205.78012749,   71.61878887,  138.13739083,\n        152.02171515,  225.44605642,   29.27688012,  116.56544273,\n        150.89887631,  114.98335065,  245.31572735,  328.53645693,\n        284.59759557,  157.78977555,  317.87473375,  102.13824116,\n         66.13636153,   27.9892651 ,  241.0817315 ,  111.65454471,\n        256.52684695,  200.3794566 ,   77.72425583,  265.72348118,\n        266.243831  ,  258.47255774,  279.68191385,   68.69718505,\n         62.04648632,  235.02637895,   73.44931819,  111.42685187,\n        110.47663071,  141.30557679,  260.17155599,  215.42719382,\n        101.07686205,  263.91319518,  282.30812068,  128.3615728 ,\n        261.35509932,  240.30897828,  232.6936401 ,   43.95802838,\n        121.98262801,  -46.18352636,   82.83295739,  193.41786386,\n        221.20253151,  -52.37626662,  118.73748972,  -74.12361549,\n        255.38233395,  269.94162531,  278.88976163,  135.38378533,\n        128.59893692,  113.09644683,  126.65226771,  232.86058918,\n         -2.27075834,  232.03478988,  282.96733419,   74.03523648,\n        150.1176627 ,  271.12477197,  162.85124063,  226.48080405,\n         27.03729883,  -41.59596432,   66.85917655,  189.95061676,\n        286.97209969,  313.35152695,  211.93407723,  310.58606832,\n        342.40879123,  -58.19745099,  231.23970817,  159.34769435,\n         12.17310707,  116.00354368,   82.41735773,  189.58909829,\n        161.88913677,  221.5979259 ,  191.09174172,  165.65215374,\n        235.88266679,  186.8404996 ,  341.78519738,  -71.69823185,\n        117.16825828,  172.61808388, -139.4546751 ,  253.06529848,\n        227.91789079,   76.01932243,  310.50974902,  222.28699066,\n        160.26337948,  308.20028909,  231.44965712,  287.45526723,\n        229.6550345 ,  356.81173903,  123.65455982,   91.29383375,\n        183.92627699,  -64.26370359,  119.73239494,  178.50125743,\n        171.25131324,  314.87791766, -101.35602671,  112.18728601,\n        -91.53866591,  228.78045192, -118.15594588,   38.68629783,\n         89.33811036, -102.36816067,   62.93449599,  173.12924656,\n        231.80605143,  292.65373997,  175.48392084,  165.93771238,\n         31.40695661,  -27.72789524,  249.7619391 ,  118.11812352,\n         -9.75720902,  317.60616014,   20.23236638,   67.70153485,\n        263.28254364,  283.59926413,   55.21302675,  -17.9509375 ,\n        214.53497101,  315.89484815,  145.81791474,  124.99078887,\n         61.64454802,  309.98151808,  115.69623071,  316.71797578,\n        208.8926406 ,   19.32428992,  236.61734529,   18.11849014,\n        267.94950226,  226.18731576,  265.06641599,  146.56972411,\n        268.50494291,  252.08165109, -119.58552157,  133.35260132,\n         76.9873222 ,   18.9618986 ,  250.12630248,   21.27702003,\n        274.78629235,   55.21098094,   74.73248874,  310.40222527,\n        140.24276899,  108.72089488,  193.39643746,  237.200911  ,\n         16.50268278,  123.06258702,  229.76875978,  357.32423683,\n        164.90751634,  141.9953912 ,  292.28825064,  158.28233512,\n        146.30957878,  -37.32691241,  104.64967916,  251.22287319,\n        288.38710833,  291.95152621,  309.04704736,  210.85277061,\n        238.16888049,  234.53437124,  -23.40392993,  165.74868857,\n        324.99852741,  149.12773548,  231.73623636,  171.600429  ,\n        324.42445456,  108.23541558,   64.83560683,  213.56746051,\n          8.52375066,  313.36164142,  315.14736365,  273.07112153,\n        125.1286601 ,   88.17936759,   42.14964773,  214.42647246,\n        319.01963879,   77.98934246,  260.25922008,   16.7319268 ,\n        261.98266732,   17.28754079,  137.91251671,  226.36681173,\n         60.29171718,  320.60472015,   78.08064986,   88.3292441 ,\n        254.49666456,   94.24127971,  238.15360139,  198.50649852,\n        134.77190732,  138.87071298,  261.37451507,   -1.47779198,\n        297.45103531,  155.99156495,  154.37270658,  242.55964209,\n        110.85420959,  210.00069972, -117.66483899,   93.88863558,\n        150.17265851,  339.31131908,    8.37988996,  288.52442884,\n        -49.70021405,  231.84176257,  337.76463964,   93.45251622,\n        -30.73641453,  191.47133768,  209.0434106 ,  146.70123471,\n        -51.71222592,  242.5851312 ,   41.72635115,  -96.82316572,\n        113.46953901, -159.81874229,   73.54092106,   86.2597753 ,\n        261.57972183,  104.1371331 ,  346.88955636,  104.20877611,\n        237.17316091,  262.43794038,    6.03684572,  247.81740585,\n        242.18805488,  292.963524  ,   -2.41310101,  161.47259875,\n        -52.36386766,  232.77739286,  343.2945098 ,  184.77885843,\n        107.69760658,  159.70565804,  190.23511967,  293.43634541,\n        160.59112613,  323.52014692,  195.61845902,  194.79449218,\n        162.47416714,  113.99829785,  -54.94284762,  283.64450578,\n        197.20875168,  238.67988378,  357.20226404,  265.14713767,\n        -10.05338121,   83.97310067,  299.54886735,  -51.77958905,\n        228.51155812,  284.98685914,   37.93383211,  309.42078794,\n        100.73481785,  103.89256427,  300.50902313,  260.55611823,\n         82.49856311,    8.63189255,  -89.16764843,  250.00888937,\n        132.1637406 ,    2.31530686,   58.26140811,  256.04666014,\n         85.38509388,  295.08568266,   85.14359681,   17.06481297,\n        218.40107228,  -42.70312847,  -58.27027936,   16.81158973,\n        173.44287347,   79.77624771,  355.26463544, -124.22102013,\n         90.05970263,  236.88073309,  274.76528269,  214.92809484,\n        126.31457943,  261.18942476,  177.72905401,  105.56711929,\n          2.04348214,  191.43804726,  214.22662598, -118.77365119,\n        290.76096368,  225.22270267,  259.62470436,   94.39346675,\n        122.42821883,  315.43018433,   93.43005135,  199.22381218,\n         41.9009655 ,  117.74178119,  290.82170939,  227.86015934,\n       -165.05756924,   90.34507745,  108.19752737,  199.13236   ,\n        241.56181317,  138.22050978,  101.23369417,  237.13491582,\n         54.46826982,  372.52141791,   67.09961521,  117.72611157,\n        282.42927169,  218.74041535,  190.55328028,  185.83909421,\n        219.48037524,  141.76784857,  100.30213564,   44.87151887,\n         83.37162612,   46.4714805 ,  136.61066576,  200.84182965,\n        307.11160005,  230.50729268,   -2.35541916,   99.81231776,\n        167.62422649,  231.42344314,  272.50979988,  261.76534015,\n        161.65825335,  309.41108267,   72.23628139,   33.11663791,\n         69.63475482,    9.69027684,  309.39196774,  284.13918855,\n        182.33779596,  233.45476741,  182.97832733,  148.97149739,\n        344.9671785 ,  252.6032108 ,  242.09650377,  169.82601926,\n        143.97357884,   69.54167661,   58.25235298,   55.54996605,\n        113.07494221,  195.66674006,  291.62372721,   51.91007436,\n        239.64460104,  205.16534906,  196.38226246,  185.33789376,\n        -74.11281425,  209.59565043,  128.1515095 ,  354.48586893,\n       -194.32975619,  267.49655962,  146.30071825,  195.1414792 ,\n        150.14961181,  281.0286717 ,  267.9807429 ,   81.86242258,\n        214.73443987,  -24.53236062,  285.39657628,  233.07358241,\n        120.30122792,  241.33223127,  301.68603556,  257.42904487,\n        104.88052148,  298.15901854,  -94.82507211,  289.4692624 ,\n        213.29119332,   42.60852338,  240.89394789,  345.15541422,\n        337.79016504,  105.77374696,  178.09919818,  292.35388418,\n        166.01150362,  277.72499946,  102.07248557,   64.31997189,\n         85.03708337,  236.42486274,  190.60331492,  325.46076066,\n        231.41165339,  178.3348644 ,  135.8381583 ,  217.97520645,\n        168.21395291,  184.79116543,  242.17656654,  270.08116668,\n        301.2792292 ,  150.3434838 ,  -38.36874538,  302.18729292,\n         64.01466639,  325.02888085,  148.58308514,   36.16553809,\n        191.49054865,  236.79922442,   84.66964716,   67.77578134])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:13:44.287777Z",
     "start_time": "2024-06-21T15:13:44.281783Z"
    }
   },
   "id": "9525bd83e80ee5",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(self.alphas * self.y) @ self.kernel_func(self.X,x) + self.b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6d599c056721adf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.01302314, -0.00079364,  0.00079364, ...,  0.00074602,\n        0.00097505,  0.00081396])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.alphas * model.y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:15:08.675599Z",
     "start_time": "2024-06-21T15:15:08.671727Z"
    }
   },
   "id": "2565539505537fe8",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[2747.60037936, 2691.65263868, 2588.55710927, ..., 2905.65651177,\n        2785.0318925 , 2839.62542968],\n       [3471.33035727, 3077.35893934, 3195.77757655, ..., 2962.65789824,\n        3569.67161785, 3405.76488456],\n       [3626.59998421, 3286.26321253, 3295.53571808, ..., 3168.44028735,\n        3428.5026344 , 3337.9038238 ],\n       ...,\n       [3545.43516493, 3360.39531991, 3229.80668907, ..., 3261.25575398,\n        3523.05201928, 3404.87382227],\n       [3598.23034808, 3604.4132433 , 3546.45090507, ..., 3423.837775  ,\n        3615.09305959, 3517.72924292],\n       [3464.79928594, 3554.76281547, 3212.17744376, ..., 3274.45945418,\n        3512.76915348, 3406.64358457]])"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.kernel_func(X_train,X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:15:42.267815Z",
     "start_time": "2024-06-21T15:15:41.938465Z"
    }
   },
   "id": "4fdf2fa7ac439a27",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([2636.3739335 , 3303.2276783 , 3446.44640771, ..., 3506.38525004,\n       3802.78997489, 3521.13239304])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train @ X_test[50,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:20:38.075934Z",
     "start_time": "2024-06-21T15:20:38.051243Z"
    }
   },
   "id": "adbfdcd9e646dc0a",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([  40.19894287,  197.64387679,   30.60322996,  163.71987974,\n        185.08337237,  -43.14213741, -217.39350929,  -16.38839869,\n        -72.8552912 ,  -42.71546513,  140.23580118,  115.29150879,\n         74.51254279,  170.44183256,  -62.19014658,  134.9933154 ,\n        184.68794923,  166.09089958,  170.48573425,  200.03708758,\n        -83.65252321,  201.30634105,  177.73887863,  130.45581495,\n        112.61237161,  -19.49072756,  183.8514258 ,  199.5230587 ,\n        102.20220469,  119.03632446,   79.59384389,  139.29595958,\n         23.55506992,  157.87868426,  128.8864294 ,  192.81349324,\n        -26.72002672,  186.56515109,  -65.40111735, -260.46329051,\n        177.13028594,  -53.95296665,  158.28208726,  181.53632831,\n       -155.33588909, -253.17441855,  156.26688765,   54.64893639,\n         61.01027112,   94.28935471,  131.23071702,  -30.60454969,\n        -36.55641257,  -79.62530284,  122.53054144,  197.50862829,\n        -46.35705561, -126.7716392 ,  103.13827804,  167.9292066 ,\n        146.72715858,   17.88388073, -171.9239908 ,  197.17906283,\n        -60.88264295,  -33.12127005,  178.5778252 ,  181.48828952,\n        220.09765422,  179.94505793,  109.23490856,   32.21473368,\n         59.41497158,  210.86033367,  129.21722863,  184.26036952,\n         83.99626362, -192.65970495,  173.8819476 ,  209.8218362 ,\n       -258.0000856 ,  212.75623642,   55.86556731,  128.04216973,\n        117.24993894,   99.81537708, -100.34389636,  211.25658639,\n         83.7926791 ,  168.52940411,  147.83907961,   94.93897854,\n         61.06532233,  -99.09661245,  175.11327912,   36.87093578,\n        196.80590234,  -35.53289086,   14.68047345,  210.2664837 ,\n         24.23206528,  -37.01087128,  113.34829889,  219.13096069,\n        198.80371541,  127.38863251,  -86.67457105,  174.26511889,\n        144.08440754,  -78.75634977,   65.44401737,  -50.0164682 ,\n         91.47576329,   20.88405418,  109.95451687,  214.41786838,\n       -115.90966387, -110.72536007,    3.80834484,   73.21800727,\n         15.17486323,  184.97828032, -110.58194868,  106.76787912,\n        -29.65199812,  -72.44307077,  230.26429696, -117.0015055 ,\n        -72.78281748, -147.77665556,   92.98115291,   22.26171704,\n        -17.83147285,  110.91361161,  -23.2392673 ,  111.17742409,\n        100.29604212,  206.62237814,   18.98160103,  102.50142002,\n        156.62205123,  129.5907173 ,  -80.16662054,  164.83712064,\n        117.88408398,   54.46454894,   35.39098273,  148.96422399,\n        235.09464976, -110.98657016,  156.95509733, -233.62967012,\n        139.90879358,  175.29693885,  103.04223259,    7.35154469,\n        145.31600086, -127.65419997,  168.4280021 ,   53.58701571,\n        173.95371952,  -16.10503824,  192.95604035,  -15.33031757,\n        -14.53994   , -105.3833161 , -119.12164461, -151.18539113,\n        234.30029967,  191.09860894,  119.01161155,   80.24519223,\n       -175.94076849,   74.34619256,  159.17999145,  134.75340043,\n         59.80721502,  -91.87404051,  145.4512585 ,  -14.84817934,\n       -224.81360164,  101.37654578,  -20.95003409,   39.84011642,\n        -65.5330312 ,  172.83747733,  115.34467356,  162.34033588,\n        173.58251375,    3.11984039,  204.35535984,  -13.63395459,\n        225.21580034, -276.31069351,  170.16324085,  -25.6501092 ,\n        -16.81373727,  -88.24079408,  152.46621548,  211.92996604,\n         61.4666525 ,  135.52614964,  -84.65666903,  140.51706702,\n       -198.624327  ,   31.8201712 ,  201.00530287,  -78.16806455,\n        222.51432671,   -8.44477691,  121.37699152,  -22.07477753,\n        201.73032274,  174.88339164, -279.13425419,   40.29277603,\n         63.2631309 , -229.52390901,  180.86898629, -141.20710223,\n         41.81786962,   65.83279072,   74.97096382,  166.77182805,\n        -55.94873615,    7.30487813,  236.31541982,  152.96972727,\n         14.44158959,  -74.08763946, -153.69743087, -132.78354046,\n        141.75184954, -166.77855842,   52.3360296 ,  -42.36835035,\n       -229.03026628,  169.34838256,   16.97402244,   85.6658872 ,\n        130.20469483,  122.35465138,  -45.06942619, -169.14955435,\n       -236.19872894,   -1.06927803,  -21.83339977,  -17.36017494,\n        142.49987082,  136.06329106,  157.17826371,  178.29051314,\n        124.9068184 ,  -46.33163793,  189.49083527,   18.22654154,\n        135.38872619,  130.53142019,   -7.14990049,  156.76896521,\n        219.54924815,   81.04266216,   -8.04933912,   -6.16273863,\n         40.63700516,  165.24496645,   16.27462391,  132.65524797,\n        113.72539008,   74.17673535,  -24.69209092, -125.08500309,\n         75.32094409,  118.06431756, -148.15496445,  -70.55413818,\n        -61.96621382,  111.47543423,  214.31605804,  102.9975609 ,\n        168.63884385,  120.18485263,  -35.97784713,  208.57859078,\n        -72.7255518 ,  -42.99074876,   48.18260835,   91.39059674,\n        -92.76483764,  174.8851449 , -174.21641806, -177.07617935,\n         62.36434094,   81.07674626, -198.68845285,  176.74564133,\n        123.19303827,  104.3189896 ,  170.1919501 ,   64.05036445,\n        116.52231689,  193.62884148,  -69.90679444,  -23.21867244,\n       -193.45662487,   58.8069896 ,   14.53610835,   79.17990944,\n        -55.34769008,   32.43040854,  242.63512485,  183.38026489,\n        -68.88849659, -135.48168426,   60.52171347,  194.87561328,\n        -22.5253234 ,  -45.9423476 ,  213.52027422,   12.17700146,\n       -231.73369346,   83.76996365,  131.90290867, -186.62932263,\n         69.4891586 , -211.86572649,   59.05240488,  217.53291431,\n         64.53688661,   44.49247513,   51.40028288, -276.08447655,\n        -17.49213463,  -59.50313745,  -63.46840921,  209.28800004,\n        238.17364792,   99.65826764,  127.79514095,  198.59035779,\n        102.13597574,  203.38961121,   40.74875281,   12.77267667,\n       -182.78401029,  102.45721922,  187.34552894,  139.46795341,\n        150.2621425 , -193.54863745,   98.67807638,   35.6782645 ,\n         88.86408163,   35.61176845, -187.62404395,   66.05949862,\n        196.16630767,  133.03305499,  154.90240237,   40.71741419,\n        131.46181484,  177.47935371,  203.8254558 , -151.8820948 ,\n         49.99823055,   -2.33334858,   38.81043143,  165.12559612,\n        -22.97651503,  132.37022518,   36.54018585, -176.47061114,\n        135.34853374,  146.7723351 ,  -94.76001425, -198.67932965,\n       -139.35712861,  190.27049814,  195.75455678,   20.75001787,\n        145.34984986,  139.06718106,  212.72595112,  -52.5805006 ,\n        105.88654179,   11.46289269, -160.05199783,  130.63441838,\n        187.00314428,   71.39620288,  -79.29747879,  131.43031113,\n        160.07495335,   14.83958949, -100.21221336,   38.11865261,\n        207.50215233, -219.52487874,  167.58711526,  172.77928987,\n        186.85487772,  266.95813318,   -8.55874973,   90.6816272 ,\n        134.03332682,   75.62544479,  -49.65288478,   93.80968081,\n         32.44320955,   72.48285987, -162.58633872,  153.05738514,\n         36.11287091,  -27.89657179,  101.43081604,  201.98688697,\n         83.62173095, -215.71104109,  -30.05741903, -279.12442526,\n        192.53446786,   89.24685367,  201.1427543 ,  -79.08686894,\n        162.62481423,  -49.82234862,  198.79462042,  249.7691894 ,\n       -136.33713528,   43.95625497,  120.37486385,   15.47447923,\n          2.72444591,   24.33195474, -269.40835474,   45.39696896,\n        201.38360627,  208.99807869,  220.3960719 ,  118.49317169,\n         20.70404122,   17.73484137, -157.24985704,  181.20657449,\n        194.21880185,  157.7990394 ,   19.92711762, -136.02585205,\n          5.82750442, -121.85220129,   63.95584719,  141.51223492,\n        -67.12026121,   50.64312971,  -39.97743548,   86.90139167,\n         60.06280377, -104.82875513,  156.55508791,   12.44998336,\n       -127.54055089,   88.83622646,  -23.77550183,   15.23978643,\n        172.26023058,  172.48679554,  193.37515958,  -86.44127256,\n        164.03304559, -141.43420046,  137.43936428,  -95.47776631,\n       -179.00210814,  114.028103  ,  158.10032272,  137.55220805,\n        138.55243687,    7.90516931,   14.30454831,  128.73205636,\n        179.14882978,  176.31939252,   80.62054465,  -13.15003041,\n        223.81806418,  142.016306  ,   28.29935915,   87.44198983,\n        195.46957888,  -34.44493669,  -52.66465784,   88.44078408,\n        145.63862698,    1.50904547,  -11.9314194 ,  175.65834097,\n         51.54257033,   96.21104469,  135.87597126,   17.53571656,\n        143.44751659,  194.38974688,  -89.28858246, -153.40796273,\n         15.08688897, -280.62408337,   24.96917185,   54.60190952,\n        104.94586291,  106.12786186,   34.24532345,  134.84501232,\n         64.12125282,  -21.00866921,  -57.67503653,    8.16914882,\n         46.56059849,  -41.68691866, -119.61483027,  -81.42839911,\n         56.96129535, -229.86557251,   93.14324572,  115.49848566,\n         25.14347486,  -78.06727726,   78.61763731,  168.88631185,\n         93.20922725,  168.69372583,   74.90612562, -131.02324775,\n        114.56999651,  -43.45855219,  -75.72840214,  103.67747162,\n         57.42366218,   91.37570306,  -42.78563556,   23.7329664 ,\n         37.61729072,  111.041632  ,  -85.1275443 ,    2.16101831,\n         36.49445189,    0.57892622,  130.91130293,  214.1320325 ,\n        170.19317115,   43.38535113,  203.47030933,  -12.26618326,\n        -48.26806289,  -86.41515932,  126.67730708,   -2.74987971,\n        142.12242253,   85.97503217,  -36.68016859,  151.31905675,\n        151.83940658,  144.06813332,  165.27748943,  -45.70723937,\n        -52.3579381 ,  120.62195453,  -40.95510623,   -2.97757255,\n         -3.92779371,   26.90115237,  145.76713157,  101.0227694 ,\n        -13.32756237,  149.50877075,  167.90369625,   13.95714838,\n        146.9506749 ,  125.90455386,  118.28921568,  -70.44639604,\n          7.57820359, -160.58795078,  -31.57146704,   79.01343944,\n        106.79810709, -166.78069104,    4.3330653 , -188.52803992,\n        140.97790952,  155.53720089,  164.48533721,   20.97936091,\n         14.1945125 ,   -1.3079776 ,   12.24784328,  118.45616476,\n       -116.67518276,  117.63036545,  168.56290976,  -40.36918794,\n         35.71323828,  156.72034755,   48.44681621,  112.07637962,\n        -87.36712559, -156.00038875,  -47.54524787,   75.54619233,\n        172.56767527,  198.94710253,   97.52965281,  196.1816439 ,\n        228.00436681, -172.60187542,  116.83528375,   44.94326993,\n       -102.23131735,    1.59911926,  -31.98706669,   75.18467387,\n         47.48471234,  107.19350147,   76.6873173 ,   51.24772932,\n        121.47824237,   72.43607518,  227.38077296, -186.10265628,\n          2.76383385,   58.21365946, -253.85909953,  138.66087405,\n        113.51346637,  -38.38510199,  196.10532459,  107.88256624,\n         45.85895505,  193.79586466,  117.0452327 ,  173.05084281,\n        115.25061008,  242.40731461,    9.25013539,  -23.11059068,\n         69.52185257, -178.66812801,    5.32797051,   64.09683301,\n         56.84688882,  200.47349324, -215.76045113,   -2.21713842,\n       -205.94309034,  114.37602749, -232.5603703 ,  -75.71812659,\n        -25.06631406, -216.77258509,  -51.46992843,   58.72482214,\n        117.40162701,  178.24931555,   61.07949641,   51.53328796,\n        -82.99746781, -142.13231966,  135.35751467,    3.71369909,\n       -124.16163344,  203.20173572,  -94.17205804,  -46.70288957,\n        148.87811922,  169.19483971,  -59.19139768, -132.35536192,\n        100.13054659,  201.49042373,   31.41349032,   10.58636444,\n        -52.7598764 ,  195.57709366,    1.29180629,  202.31355136,\n         94.48821617,  -95.0801345 ,  122.21292086,  -96.28593428,\n        153.54507784,  111.78289134,  150.66199156,   32.16529968,\n        154.10051848,  137.67722666, -233.98994599,   18.9481769 ,\n        -37.41710222,  -95.44252583,  135.72187805,  -93.12740439,\n        160.38186793,  -59.19344348,  -39.67193568,  195.99780084,\n         25.83834457,   -5.68352954,   78.99201304,  122.79648657,\n        -97.90174165,    8.6581626 ,  115.36433536,  242.91981241,\n         50.50309192,   27.59096677,  177.88382622,   43.8779107 ,\n         31.90515436, -151.73133683,   -9.75474526,  136.81844876,\n        173.98268391,  177.54710179,  194.64262294,   96.44834619,\n        123.76445607,  120.12994681, -137.80835435,   51.34426415,\n        210.59410299,   34.72331106,  117.33181194,   57.19600458,\n        210.02003014,   -6.16900884,  -49.56881759,   99.16303609,\n       -105.88067377,  198.95721699,  200.74293922,  158.66669711,\n         10.72423568,  -26.22505683,  -72.25477669,  100.02204804,\n        204.61521437,  -36.41508197,  145.85479566,  -97.67249763,\n        147.5782429 ,  -97.11688363,   23.50809229,  111.96238731,\n        -54.11270724,  206.20029572,  -36.32377457,  -26.07518032,\n        140.09224013,  -20.16314471,  123.74917697,   84.10207409,\n         20.36748289,   24.46628856,  146.97009065, -115.8822164 ,\n        183.04661088,   41.58714053,   39.96828216,  128.15521767,\n         -3.55021483,   95.5962753 , -232.06926341,  -20.51578884,\n         35.76823408,  224.90689466, -106.02453446,  174.12000442,\n       -164.10463848,  117.43733814,  223.36021521,  -20.9519082 ,\n       -145.14083895,   77.06691325,   94.63898618,   32.29681029,\n       -166.11665034,  128.18070678,  -72.67807327, -211.22759014,\n         -0.93488541, -274.22316671,  -40.86350337,  -28.14464912,\n        147.1752974 ,  -10.26729133,  232.48513194,  -10.19564831,\n        122.76873649,  148.03351596, -108.3675787 ,  133.41298142,\n        127.78363046,  178.55909958, -116.81752543,   47.06817433,\n       -166.76829209,  118.37296844,  228.89008537,   70.37443401,\n         -6.70681784,   45.30123362,   75.83069525,  179.03192098,\n         46.18670171,  209.11572249,   81.2140346 ,   80.39006775,\n         48.06974272,   -0.40612657, -169.34727204,  169.24008136,\n         82.80432726,  124.27545936,  242.79783962,  150.74271325,\n       -124.45780563,  -30.43132375,  185.14444293, -166.18401348,\n        114.1071337 ,  170.58243471,  -76.47059231,  195.01636352,\n        -13.66960657,  -10.51186015,  186.10459871,  146.15169381,\n        -31.90586131, -105.77253187, -203.57207286,  135.60446495,\n         17.75931618, -112.08911756,  -56.14301631,  141.64223572,\n        -29.01933054,  180.68125824,  -29.26082761,  -97.33961145,\n        103.99664785, -157.1075529 , -172.67470378,  -97.59283469,\n         59.03844904,  -34.62817671,  240.86021102, -238.62544456,\n        -24.34472179,  122.47630867,  160.36085827,  100.52367041,\n         11.91015501,  146.78500034,   63.32462959,   -8.83730514,\n       -112.36094228,   77.03362284,   99.82220156, -233.17807561,\n        176.35653926,  110.81827825,  145.22027994,  -20.01095768,\n          8.02379441,  201.02575991,  -20.97437308,   84.81938776,\n        -72.50345892,    3.33735677,  176.41728497,  113.45573491,\n       -279.46199367,  -24.05934697,   -6.20689705,   84.72793558,\n        127.15738875,   23.81608535,  -13.17073026,  122.7304914 ,\n        -59.9361546 ,  258.11699349,  -47.30480921,    3.32168715,\n        168.02484726,  104.33599093,   76.14885586,   71.43466979,\n        105.07595082,   27.36342415,  -14.10228879,  -69.53290555,\n        -31.0327983 ,  -67.93294392,   22.20624134,   86.43740523,\n        192.70717563,  116.10286825, -116.75984358,  -14.59210667,\n         53.21980206,  117.01901871,  158.10537546,  147.36091572,\n         47.25382893,  195.00665825,  -42.16814303,  -81.28778652,\n        -44.7696696 , -104.71414759,  194.98754331,  169.73476413,\n         67.93337154,  119.05034298,   68.57390291,   34.56707297,\n        230.56275408,  138.19878638,  127.69207934,   55.42159484,\n         29.56915442,  -44.86274781,  -56.15207145,  -58.85445838,\n         -1.32948221,   81.26231564,  177.21930278,  -62.49435006,\n        125.24017662,   90.76092464,   81.97783803,   70.93346934,\n       -188.51723868,   95.191226  ,   13.74708508,  240.0814445 ,\n       -308.73418062,  153.0921352 ,   31.89629382,   80.73705478,\n         35.74518739,  166.62424727,  153.57631848,  -32.54200184,\n        100.33001545, -138.93678504,  170.99215185,  118.66915799,\n          5.8968035 ,  126.92780684,  187.28161114,  143.02462044,\n         -9.52390294,  183.75459411, -209.22949654,  175.06483797,\n         98.8867689 ,  -71.79590104,  126.48952347,  230.7509898 ,\n        223.38574062,   -8.63067746,   63.69477376,  177.94945976,\n         51.6070792 ,  163.32057504,  -12.33193885,  -50.08445253,\n        -29.36734105,  122.02043832,   76.1988905 ,  211.05633624,\n        117.00722897,   63.93043997,   21.43373388,  103.57078202,\n         53.80952849,   70.386741  ,  127.77214212,  155.67674226,\n        186.87480478,   35.93905938, -152.7731698 ,  187.7828685 ,\n        -50.38975803,  210.62445642,   34.17866072,  -78.23888634,\n         77.08612423,  122.3948    ,  -29.73477726,  -46.62864309])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.alphas * model.y) @ model.kernel_func(X_train,X_test) - model.b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:25:10.820540Z",
     "start_time": "2024-06-21T15:25:10.565553Z"
    }
   },
   "id": "423370fab37de2e7",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.02077156,  0.00629244, -0.44885489, ..., -0.07390252,\n       -0.00827287, -0.42565927])"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:32:25.814324Z",
     "start_time": "2024-06-21T15:32:25.810709Z"
    }
   },
   "id": "feb74c8e6ba1ccd4",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "np.float64(0.02077155735911605)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:36:25.035762Z",
     "start_time": "2024-06-21T15:36:25.032399Z"
    }
   },
   "id": "13cc8ba769cb1695",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.02077156,  0.00629244, -0.44885489, ..., -0.07390252,\n       -0.00827287, -0.42565927])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is w for linear\n",
    "(model.alphas * model.y) @ X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:36:43.004113Z",
     "start_time": "2024-06-21T15:36:42.991462Z"
    }
   },
   "id": "8d73f47d4d6549cc",
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c58c2fe05ba53a5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
